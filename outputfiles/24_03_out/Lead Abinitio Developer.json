{"text": "Name: margaret margaret E-Mail: margaret.margaret@gmail.com Address: Chengdu, China Github: https://github.com/margaret LinkedIn: https://linkedin.com/margaret Phone No. 353867985203 PROFESSIONAL SUMMARY:  9+Years of professional work experience in analysis, design, development, data conversion and ETL (Ab Initio) experience with strong industry experience includes Banking, HealthCare. Well known with the concept, design and development of DataMarts, Data Warehouse using Star Schema and Snowflake Schema in implementing Decision Support System. Technically worked extensively on Teradata\u00a0utilities like Fast Load, Multi Load, Fast Export, TPump and BTEQ. Have exposure to work on mainframe systems.\u00a0  Proficient in coding\u00a0JCL\u00a0scripts in Mainframe systems. Strong Data warehousing experience using Ab Initio 2.13/2.14/2.15, Unix Shell scripting, Informatica, Oracle 10g/9i/8i, OLTP, OLAP. Proficient with various AbInitio parallelism techniques and implemented AbInitio Graphs using Data parallelism and MFS techniques. Configured graph parameters, sandbox parameters, environment variables and EME Repository environment for Production/Development and performance tuning for load graph process.  Experience in design, development, data loading and data extractions for Oracle 8i/9i/10g , Sybase, Teradata databases, DB2UDB, DB2MF, Flatfiles, SQL Server and Mainframe VSAM Files. Designed and encouraged use of Reusable graphs in Ab Initio to promote reusability, eliminate coding redundancy and ease maintenance of Version control. Responsible for Debugging and Troubleshooting Ab Initio graphs. Extensive experience in writing UNIX shell scripts. Extensive experience in creating the Graph specification Documents to develop the Graphs, Created Unit Test cases and Test plans. Experience in COBOL. Involved in Risk management, Identification, assessment and prioritization of risks. Experience in running Mainframe JCL jobs. Hands of experience in Ab Initio Testing. Used Control-M and Tivoli schedulers to schedule jobs Experience in creating the Technical Documentation. Well versed with Abinitio Architecture. Worked under extremely tight deadlines. Diverse background with fast Learning and creative Analytical abilities with good Technical, Communication and Interpersonal skills. Perform peer review on design, code and unit testing. Provide work product delivery support (test product, post implementation support) TECHNICAL SKILLS:  ETL Tool                : \tAb Initio 3.1.6.2/2.13/2.14/2.15, Informatica 6.x, Data stage OLAP Tool             : Business Objects 5/XI, Cognos Database                 : Oracle 8x/9x/10g/11g, Teradata 12, Sybase IBM DB2, DB2MF, DB2UDB,                                      SQL Server GUI Tool                 : Toad 7.x Languages              : \tSQL, PL/SQL, Unix Shell scripting, COBOL, Mainframes, Java, .Net Operating Systems: MS Windows (2000, NT, XP), UNIX Rational Tools        : ClearCase, Clearquest, CVS, Ant PROFESSIONAL EXPERIENCE:  Harte-Hanks, Billerica, MA\t\t\t\t\t         \t\t     Oct 2013 \u2013 Till date Lead Abinitio Developer Project Description: Insight-driven multi-channel marketing solutions to build the relationships that impact some of the world\u2019s best-known B2C and B2B brands. Use the intelligence to build carefully crafted, multi-channel customer experiences that speak directly to prospects at critical moments along the path to purchase and throughout their customer experience. Building relationship in Brand Loyalty.\u00a0 Roles & Responsibilities:  Understanding the specifications for Data Warehouse ETL Processes and interacting with the data analysts and the end users for informational requirements. Development of Ab Initio applications for L\u2019Oreal Project.  Working closely with Business Analysts to interpret the Business rules and make necessary modifications to process the data for accuracy.  Used Ab Initio GDE to generate complex graphs for transformation and Loading of data into Staging and Target Data base area. Responsible for day-to-day support of extracting data from major database sources Developed and supported the extraction, transformation and load process (ETL) for a Data Warehouse from their OLTP systems using Ab Initio and provide technical support and hands-on mentoring in the use of Ab Initio.  Modified the Ab Initio graphs to utilize data parallelism and thereby improve the overall performance to fine-tune the execution times. Environment: Ab Initio GDE 3.1.6.2, Co>Operating System 3.1.6.5, Oracle 9i, UNIX,  Control-M Southwest Airlines, Dallas, TX\t\t\t\t\t         \t\t   Aug 2011 \u2013 Sep 2013 Lead Abinitio Developer Project Description: This Project dealt with building an Enterprise Data Warehouse to build the capability to analyze the customer details. The data was extracted from various Heterogeneous Data Sources like Oracle, Mainframes and Flat files. The Target Data Warehouse was built on Teradata.  Roles & Responsibilities:  Responsible for requirements gathering, project formalization, project scoping. Design reviews and walkthrough of all technical deliverables. Developed Abinitio interface graphs as per the business requirement to load the warehouse data. Used Ab Initio graphs to load data into the Teradata database. Improved performance of existing Ab Initio graphs. Testing activities under various environments. Responsible for planning and managing the work pipeline to ensure steady utilization towards the project. Analysis of the specifications provided by the clients, Preparation of Design Docs, Test Case, and Implementation documents. Testing the graph developed in the production environment. Implementing them in the production environment Impact analysis for any changes or new implementation on production. Environment: Ab Initio GDE 1.14, Co>Operating System 2.14, DB2, Teradata, UNIX, Windows XP. RadioShack, Fort Worth, TX\t\t\t\t\t         \t\t      Nov 2010 \u2013 Aug 2011 Abinitio Developer Project Description: RadioShack Corporation (NYSE: RSH) is one of the nation\u2019s most experienced and trusted consumer electronics specialty retailers. The company has a presence through almost 6,000 company-operated stores and dealer outlets in the United States, more than 150 RadioShack locations in Mexico and nearly 800 wirelesses Phone kiosks. DCM: Currently legacy system feeds the model quantity and rate of sale details to replenishment and other downstream systems for all product categories. As part of this project, for products replenished in DCM, a new program will be created to send the rate of sale and derived model quantity details from DCM to Mainframe. Recurring Billing: Service Plan process will check Recurring Billing database and will get any recurring billing extensions made on Service Plan accounts.    RadioShack sends the installment extensions each day, if extension is not received by the payment due date, the Service Plan can be cancelled at Assurant.  There is a 15 day \u2018grace period\u2019 on the installment payment due date before the service plan is cancelled from Assurant.\t\t\t\t Roles & Responsibilities:  Extracts RSSP (RadioShack Service Plan) recurring billing installments data from the recurring billing repository and the RSSP transactional database.   Sends the installments information to Assurant using the same file format as that used to send initial sales/refunds to Assurant. Responsible for requirements gathering, project formalization, project scoping. Design reviews and walkthrough of all technical deliverables. Developed Abinitio interface graphs as per the business requirement to load the warehouse data. Used Ab Initio graphs to load data into the DB2 and Teradata database. Improved performance of existing Ab Initio graphs. Testing activities under various environments. Responsible for planning and managing the work pipeline to ensure steady utilization towards the project. Analysis of the specifications provided by the clients, Preparation of Design Docs, Test Case, and Implementation documents. Testing the graph developed in the production environment. Implementing them in the production environment Impact analysis for any changes or new implementation on production. Solving business tickets, interaction with client and business users. Created various test plans to validate the results. Actively involved in gathering the feedback from the users to develop strategies to further tweak the system to achieve better results Environment: Ab Initio GDE 1.15.6.2, Co>Operating System 2.15, DB2, Oracle 9i, Teradata, UNIX, Windows XP. FedEx, Colorado Springs, CO\t\t\t\t\t                                May 2010 \u2013 Oct 2010 Abinitio Developer Project Description: Worked as Senior ETL Developer in the Actual Metrics, Performance Reviews, and Waivers project for FCIS Data and Reporting Warehouse initiative which is focused on delivering an enterprise data warehouse designed to consolidate, enhance and streamline current data collection and reporting efforts within FCIS related to customer service agent performance. This initiative will replace the existing reporting environment including its data sources and processing. It will also bring in additional data sources not currently available. Roles & Responsibilities:  Developed complex Ab Initio graphs incorporating complex business transformations. Implemented Ab Initio\u2019s parallelism for faster processing of data. Performed unit and system testing of the Ab Initio graphs. Used Ab Initio to read from multiple sources like Teradata, Oracle, and SQL Server. Used Ab Initio graphs to load data into the Teradata database. Developed parameter sets for the Ab Initio graphs to run a graph with different inputs. Improved performance of existing Ab Initio graphs. Created plans to automate the process using Plan>It Developed plans to create the production system consisting of various Ab Initio graphs. Environment: Ab Initio GDE 1.15.12, Co>Operating System 2.15, Ab Initio Conduct>It, Teradata, Oracle 9i, SQL Server 2008, DB Visualizer, Windows XP, Source Forge. Bank of America, Dallas, TX                                                        \t                  Nov\u201908 to Apr\u20192010 Abinitio Developer Project Description: UDAP - Unfair and deceptive Acts and Practices is a mandatory project for all the banks in United States. The proposed UDAP rule addresses practices that have raised concern about fairness and transparency and it is a rule to outline requirements on unfair or deceptive acts or practices for federal savings associations. The goal is to provide greater transparency, expectations on sound consumer protections and continued adequate oversight with respect to unfair and deceptive acts and practices. The Data Management team was responsible for preparing the test data for the UDAP application. The data management team captures live data from production every three months. And this data is subsetted, fictionalized with Retail testing platforms.  Roles & Responsibilities:  Generated graphs in Ab Initio to subset and fictionalize the data in DB2 tables and Mainframe files (GDG, VSAM and flat files). Extensively used the following Ab Initio components:\u00a0MVS Input File, MVS Output File, Input Table, Output Table, Input File, Output File, Lookup File, Filter by Expression, Reformat, Join, Sort, Dedup Sort, Split, Replicate. Designed, developed and Unit tested Ab Initio graphs using GDE for Extraction, Transformation and Loading of data from source to target Generated various Reports and Extracts for analysis purposes. Worked with tables/files containing\u00a0vectors. Generated\u00a0parameterized graphs\u00a0and wrote\u00a0shell scripts\u00a0to automate the running of these graphs. Worked on\u00a0TSO SIM\u00a0Tool in\u00a0Mainframes. Modified and ran\u00a0SAS\u00a0jobs. Generated\u00a0GDG, VSAM\u00a0and\u00a0Flat files\u00a0in Mainframes. Worked on\u00a0DB2\u00a0database. Used\u00a0QMF\u00a0to run the\u00a0SQL\u00a0queries. Performed data cleansing by developing various AbInitio Graphs and used AbInitio functions like is_valid, is_defined, is_error, sting_substring, srting_concat and other string_* functions Worked in EME sandbox environment to obtain data and variables by checking them out from the repository. The graphs and data were checked into the repository for common use by the other team members and to maintain version control Worked on improving the performance of AbInitio graphs by using Various AbInitio performance techniques like using lookups (instead of joins), In-Memory Joins, In \u2013Memory sorts and rollups to speed up various AbInitio Graphs.  Created a Production support document and documented the Test case work book, High level Design and Detail Design documents Timely Resolution of critical production issues and bug fixing. Impact analysis for any changes or new implementation on production Provided\u00a024*7 Production support Environment: Ab Initio (GDE 1.14.4.1, Co>operating system 2.14), Oracle 11g, Teradata, DB2, QMF, Sybase, UNIX IBM AIX, Shell scripts, IBM Mainframes, TSO SIM Tool, GDG, VSAM, Flat files, JCL, SAS, CVS. FEPOC, Washington, DC\t\t      \t\t\t\t\t         May\u201908 to Oct\u201908 Abinitio Developer Project Description: The FEP Director's Office (FEPDO) needs the FEP Operations Center (FEPOC) electronically exchange FEP enrollment information and Medicare entitlement information between the Centers for Medicare & Medicaid Services (CMS) and the FEPOC.\u00a0  Roles & Responsibilities:  Involved in understanding the Requirements of the end Users/Business Analysts and Developed Strategies for ETL processes.  Responsible for the detailed design and documentation Provided technical solutions for the Process requests raised by Data team to fix the issues in the existing system. Extensively used Database and Dataset components like Input file, Input table, and Output table and transform components like Join, Rollup, Scan, Filter by expression, Reformat and other components like Merge, Lookup, Input/Output table, Dedup and Sort Extensively involved in performing EME dependency analysis Used Partition components like partition by expression, partition by key, etc., to run the middle layer processing parallel. Extensively used various inbuilt transform functions like string_substring, string_lpad, string_index, lookup functions, date functions, error functions. Utilized multi file system (MFS) to execute the graph parallel. Worked on improving performance of Ab Initio graphs by using various Ab Initio performance techniques like using lookups, in memory joins and rollups to speed up various Ab Initio graphs. Designed and developed parameterized generic graphs Extensively worked in the UNIX environment using Shell Scripts.  Created test cases and performed unit testing for the Ab Initio graphs. Documented Unit testing. Logged and resolved defects in the roll out phase. Responsible for supporting the CM team and troubleshooting any production issues. Created a Production support document and documented the Test case work book, High level Design and Detail Design documents Closely monitored the Control-M batch jobs in ETL batch run during System, Integration and Acceptance test runs Environment: Ab Initio GDE 1.15.1.5, Co>Operating System 2.15, Control M, Oracle 10g, Unix Shellscripting, DB2, Windows XP, SQL Plus, Business Objects, Rational ClearCase, Rational ClearQuest, Cognos SunTrust Bank, IBM Hyderabad, India\t\t      \t\t\t          Jul\u201906 to Mar\u201908 Abinitio Developer Project Description: The TPR 3 project mission is to expand upon the work done in the TPR 1 and TPR 2 projects to acquire and store operational data from additional 65 SunTrust source systems. Business benefits of this project are: Providing platform for data exploration and profiling, Serving a base for Operational Reporting via LOB sandboxes, and preparing source files for EDW and FDW Roles & Responsibilities:  Analyze the design requirements and making the ETL flow accordingly. Developed number of AbInitio Graphs based on business requirements using various AbInitio components like Partition by Key, Partition by round robin, reformat, rollup, join, scan, normalize, gather, replicate, merge etc Performed data cleansing by developing various AbInitio Graphs and used AbInitio functions like is_valid, is_defined, is_error, sting_substring, srting_concat and other string_* functions Worked in EME sandbox environment to obtain data and variables by checking them out from the repository. The graphs and data were checked into the repository for common use by the other team members and to maintain version control Partition components like Partition by Key and Partition by Expression were used to partition the data files to facilitate parallel operations on the partitioned datasets on different nodes and thereby use the parallelism features of AbInitio to good effect Worked on improving the performance of AbInitio graphs by using Various AbInitio performance techniques like using lookups (instead of joins), In-Memory Joins, In \u2013Memory sorts and rollups to speed up various AbInitio Graphs.  Created Ab Initio Reusable graphs. Created scripts for automating the process. Worked concertedly with an offshore team to build the project and undertook responsibilities of working closely with users for requirements gathering and providing the offshore team with detailed requirements documents Making some quality work like making metrics analysis, and doing defection prevention. Environment: AbInitio Co>Op 2.14, GDE 1.15, IBM DB2 8.2 and HP UX (Unix Shell Scripting), DB2UDB, Oracle 9i, DB2MF, Flatfiles, SQL Server, Mainframe VSAM Files, Sybase, Cobol, CA-7, Cognos Unilever, Bangalore, India.\t\t          \t\t  \t\t \t       May \u201904 to July\u201906 ETL Developer Project Description: The Home and Personal Care - Europe (HPC-E) division is one of six Global Home and Personal Care divisions within Unilever Plc. This project involved the design and development of Enterprise-wise Sales Data Mart. The scope of this project is to maintain present and historic data of customers to support the client\u2019s business process to enable higher management to take decisions to forecast the future market scenario. Roles & Responsibilities:  Involved in Extraction of data from Oracle using Informatica Power Center Used Informatica tools such as Designer and Work flow Manager for Extraction,    Transformation and Loading Extensively used Informatica Power center and created mappings using transformations to flag the record using update strategy for populating the desired slowly changing dimension tables Used Informatica Source Analyzer, Mapping Designer, Transformation Developer and   Warehouse Designer for Extraction, Transformation and Loading Involved in Unit Testing and Performance tuning Environment: Informatica PowerCenter 6.1, Oracle 9i, Windows 2000 Professional EDUCATION:   Master of Science in Information Systems MSc (IS), Osmania University, Hyderabad, 2004"}