{"text": "Name: mary mary E-Mail: mary.mary@gmail.com Address: Los Angeles, United States Github: https://github.com/mary LinkedIn: https://linkedin.com/mary Phone No. 146208984000                                   Professional Summary:\t\t\t Overall 6+ years of IT experience in analysis, design, development and implementation of business applications with thorough knowledge in\u00a0Java, J2EE, Big Data,\u00a0Hadoop\u00a0EcoSystem and RDBMS related technologies with domain exposure in Retail, Healthcare, Banking, E-commerce websites, Insurance, Logistics and Financial (Mortgage) systems.\u00a0 Expertise with the tools in Hadoop Ecosystem including Pig, Hive, HDFS, MapReduce, Sqoop, Storm, Spark, Kafka, Yarn, Oozie, and Zookeeper.  Excellent knowledge on Hadoop Architecture such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node and MapReduce programming paradigm. Strong experience on Hadoop distributions like Cloudera, MapR and HortonWorks.  Good Knowledge on\u00a0Hadoop\u00a0Cluster architecture and monitoring the cluster.\u00a0 Hands on experience in installing, configuring Cloudera Apache\u00a0Hadoop\u00a0ecosystem components like Flume, Hbase, Zoo Keeper, Oozie, Hive, Sqoop and Pig. Experience in developing MapReduce Programs using Apache Hadoop\u00a0for analyzing the big data as per the requirement.\u00a0\u00a0 Highly capable of processing large sets of Structured, Semi-structured and Unstructured datasets supporting Big Data applications.\u00a0 Extensive hold over Hive and Pig core functionality by writing Pig Latin UDFs in Java and used various UDFs from Piggybanks and other source.\u00a0 Good experience in Hive partitioning, bucketing and perform different types of joins on Hive tables and implementing Hive Sere like JSON and ORC.\u00a0 Worked on different file formats (ORCFILE, TEXTFILE) and different Compression Codecs (GZIP, SNAPPY, LZO). Proficiency in\u00a0Hadoop\u00a0data formats like AVRO & Parquet. Comprehensive knowledge and experience in process improvement, normalization/de-normalization, data extraction, data cleansing, data manipulation on Hive. Have good knowledge on NoSQL databases like HBase, Cassandra and MongoDB. Proficient in implementing HBase. Used Zookeeper to provide coordination services to the cluster. Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph (DAG) of actions with control flows. Experience using Sqoop to import data into HDFS from RDBMS and vice-versa. Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka. Implemented indexing for logs from Oozie to\u00a0Elastic\u00a0Search. Analysis on integrating Kibana with\u00a0Elastic\u00a0Search. Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using Scala. Developed Apache Spark jobs using Scala in test environment for faster data processing and used Spark SQL for querying. Experience in creating Spark Contexts, Spark SQL Contexts, and Spark Streaming Context to process huge sets of data. Exploring with the Spark for improving the performance and optimization of the existing algorithms in\u00a0Hadoop\u00a0using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN. Experienced in Spark Core, Spark RDD, Pair RDD, Spark Deployment Architectures. Extensive experience using MAVEN and ANT as a Build tool for the building of deployable artifacts from source code.\u00a0 Worked with Big Data distributions like Cloudera (CDH 3, 4 and 5) with Cloudera Manager.\u00a0 Knowledge on Cloud technologies like AWS Cloud and Amazon Elastic Map Reduce (EMR). Experience working with Amazon Web Services S3, EC2, Redshift. Proficient in using OOPs Concepts (Polymorphism, Inheritance, Encapsulation) etc.  Good knowledge in advanced java topics such as Generics, Collections and multi-threading. Experience in database development using SQL and PL/SQL and experience working on databases like Oracle 9i/10g, SQL Server and MySQL.  Data Warehouse experience using Informatica Power Center as ETL tool.\u00a0 Experience in using data integration tool like Talend. Experience in performance tuning and fine tuning of a hadoop cluster. Experience with cloudera Manager and Ambari on cloudera and Hortonworks clusters.\u00a0 Experience in Unix/Linux shell scripting. Worked with Admin teams to setup cluster securities like Kerberos, Ranger, and Knox.  Experience in Configuring, managing & tuning of HDFS with MapReduce (v1)/Yarn (v2) architecture with High- Availability. Excellent interpersonal skills, good experience in interacting with clients with good team player and problem solving skills. Strong knowledge in development of Object Oriented and Distributed applications. Written unit test cases using JUnit and MRUnit for Map Reduce jobs.  Proficiency in\u00a0Hadoop\u00a0data formats like AVRO & Parquet. Experience with code development frameworks \u2013 GitHub, Jenkins.  Deployed and monitored scalable infrastructure on cloud environment Amazon web services (AWS). \u00a0Knowledge in Machine Learning (Linear Regression, logistic regression, Clustering, Classification, and Decision Tree, support vector machines and dimensionality reduction). Comprehensive knowledge of Software Development Life Cycle (SDLC), having thorough understanding of various phases like Requirements Analysis, Design, Development and Testing.  Involved in the Software Life Cycle phases like Agile and Waterfall estimating the timelines for projects.  Ability to quickly master new concepts and applications.  Technical skills:\t\t\t Big Data Technologies: Hadoop (HDFS & MapReduce), PIG, HIVE, HBASE, ZOOKEEPER, Sqoop, Apache Storm, Flume, Kafka, Spark, Spark Streaming, Mlib, Spark SQL and Data Frames, Graph X, Scala, Solr, Lucene, Elastic Search and AWS Programming & Scripting Languages: Java, C, SQL, R, Python, Impala, Scala, C++ J2EE Technologies:  JSP, SERVLETS, EJB, Angular JS Web Technologies:  HTML, JavaScript Frameworks:  Spring 3.5 \u2013 Spring MVC, Spring ORM, Spring Security, Spring ROO, Hibernate, Struts.  Application Servers:  IBM Web Sphere, JBoss WebLogic Web Servers:  Apache Tomcat Databases: MS SQL Server & SQL Server Integration Services\u00a0(SSIS), My SQL, MongoDB, Cassandra, Oracle DB, Teradata Designing Tools:  UML, Visio IDEs: Eclipse, Net Beans Operating System: Unix, Windows, Linux, Cent OS Others: Putty, WinScp, DataLake, Talend, Tableau, GitHub, SVN, CVS. Hadoop/spark Developer Project: \u00a0Hilton World Wide, TN                                                                                               Dec 2015 \u2013 Current Description: Hilton is one of the largest and fastest growing hospitality companies in the world. We are Designing an analytical platform with a distributed cloud ingestor for data intake weblogs, reviews, complaint logs on cloudera CDH cluster with spark as the distributing computing engine.\u00a0 Responsibilities: Working on Big Data infrastructure for batch processing as well as real-time processing. Responsible for building scalable distributed data solutions using Hadoop. Involved in creating\u00a0Hive\u00a0Tables, loading with data and writing\u00a0Hive\u00a0queries which will invoke and run Map Reduce jobs in the backend.\u00a0 Designed and implemented Incremental Imports into\u00a0Hive\u00a0tables.\u00a0 Very good understanding of Partitions, Bucketing concepts in\u00a0Hive\u00a0and designed both Managed and External tables in\u00a0Hive\u00a0to optimize performance. Experience in importing and exporting tera bytes of data using Sqoop from HDFS to Relational Database Systems and vice-versa.\u00a0 Moved Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables.  Written\u00a0Hive\u00a0jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data. Experience in manipulating/analyzing large datasets and finding patterns and insights within structured and unstructured data. Involved in collecting, aggregating and moving data from servers to HDFS using Apache Flume.\u00a0 Experienced in managing and reviewing the\u00a0Hadoop\u00a0log files.\u00a0 Involved in developing Pig Scripts for change data capture and delta record processing between newly arrived data and already existing data in HDFS. Migrated ETL jobs to\u00a0Pig\u00a0scripts do transformations, even joins and some pre-aggregations before storing    the data onto HDFS.\u00a0 Implemented the workflows using Apache Oozie framework to automate tasks.  Used Zookeeper to co-ordinate cluster services. Worked on different file formats like Sequence files, XML files and Map files using Map Reduce Programs.  Used Impala where ever possible to achieve faster results compared to Hive during data Analysis.  Implemented data ingestion and handling clusters in real time processing using Kafka. Worked on writing transformer/mapping Map-Reduce pipelines using Java.\u00a0 Developed scripts and automated data management from end to end and sync up between all the clusters.\u00a0 Transform the logs data into data model using apache pig and written UDF\u2019s functions to format the logs data.  Experience in designing and developing applications in Spark using Scala to compare the performance of Spark with Hive and SQL/Oracle. Involved in converting Hive/SQL queries into\u00a0Spark\u00a0transformations using\u00a0Spark\u00a0RDDs, Python and Scala.  Experience in both SQLContext and SparkSession Experienced in working with spark eco system using Spark SQL and Scala queries on different formats like Text file, CSV file.  Used\u00a0Spark\u00a0API over\u00a0Hadoop\u00a0YARN to perform analytics on data in Hive. Developed and Configured Kafka brokers to pipeline server logs data into spark streaming. Developed Spark scripts by using scala shell commands as per the requirement. Developed spark code and spark-SQL/streaming for faster testing and processing of data.  Experience in implementing Log Error Alarmer in Spark Exported the analyzed data to relational databases using sqoop for visualization and to generate reports. Experienced in Monitoring Cluster using Cloudera manager. Collaborated with the infrastructure, network, database, application and BI teams to ensure data quality and availability.\n Environment:\u00a0Hadoop, HDFS, Pig, Apache Hive, Sqoop, Flume, Kafka, Apache Spark, Storm, Solr, Shell Scripting, HBase, Scala, Python, Kerberos, Agile, Zoo Keeper, Maven, AWS, MySQL. Hadoop Developer Client: Macy\u2019s Inc., OH                                                                                                       July 2014 \u2013 Nov 2015                                 Description: As the fastest growing part of Macy\u2019s Inc. business, macys.com is achieving record sales. We build data products and data systems and work on collecting, storing, processing and analyzing of huge sets of sales data. And managing and organizing the core domains such as Product, Price, Sales, Fulfillment, Inventory, Customer, Offers and Associates.    Responsibilities: Developing and running Map-Reduce jobs on YARN and\u00a0Hadoop\u00a0clusters to produce daily and monthly reports as per user's need.\u00a0 Worked on analyzing, writing Hadoop MapReduce jobs using Java API, Pig and Hive.\u00a0 Imported data using Sqoop to load data from MySQL to HDFS on regular basis.  Implemented data access jobs through Pig, Hive, Tez, Solr, Accumulo, Hbase, and Storm. Worked on Developing custom MapReduce programs and User Defined Functions (UDFs) in Hive to transform the large volumes of data with respect to business requirement.  Extending HIVE and PIG core functionality by using custom User Defined Function\u2019s (UDF), User Defined Table-Generating Functions (UDTF) and User Defined Aggregating Functions (UDAF) for Hive and Pig using python. Developed HIVE scripts for analyst requirements for analysis. Involved in loading and transforming large sets of Structured, Semi-Structured and Unstructured data and analyzed them by running Hive queries and Pig scripts.\u00a0 Analyzed the data by performing Hive queries (HiveQL), Impala and running Pig Latin scripts to study customer behavior.\u00a0 Processed HDFS data and created external tables using Hive and developed scripts to ingest and repair tables that can be reused across the project. Implemented data ingestion and handling clusters in real time processing using Kafka. Filter the dataset with PIG UDF, PIG scripts in HDFS and Storm/Bolt in Apache Storm. \u00a0Involved in writing Pig Scripts for Cleansing the data and implemented Hive tables for the processed data in tabular format. \u00a0Involved in collecting, aggregating and moving data from servers to HDFS using Apache Flume. \u00a0Written Map Reduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase-Hive Integration.\u00a0 Experienced with performing CURD operations in HBase. Used JSON, Parquet and Avro SerDe's for serialization and de-serialization. Setting up CRON job to delete\u00a0hadoop\u00a0logs/local old job files/cluster temp files. Using HBase to store majority of data which needs to be divided based on region. Used Maven extensively for building jar files of MapReduce programs and deployed to cluster. Used Zookeeper to provide coordination services to the cluster. Experienced in managing and reviewing\u00a0Hadoop\u00a0log files.  Hands-on expertise with various architectures in MongoDB & CassandraDB.  Very good experience in monitoring and managing the Hadoop cluster using\u00a0Hortonworks. Used Amazon Redshift for data warehouse and to generate backend reports.  Exported the business required information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data.        Environment:\u00a0Hadoop, HDFS, Pig, Hive, HBase, Map Reduce, Sqoop, Flume, Impala, Oozie, Zookeeper,      LINUX, BigData, Java, Eclipse, Maven, SQL, Ambari, NoSql. Hadoop Developer Client: Alten calsoft labs, CA                                                                                                    Nov 2012 \u2013 Sep 2013 Description: Alten Group is a leading provider of comprehensive health informatics solutions for more than 200+ hospitals in the US. The customer wanted AltenCalsoft Labs to identify a patient's chance of getting re-admitted upon discharge within 30 days. So, Clinicians can be prepared to provide better post-discharge care for patients who are likely to get re-admitted. Responsibilities: Worked with Big Data team responsible for building\u00a0Hadoop\u00a0stack and different big data analytic tools, migration from RDBMS to\u00a0Hadoop\u00a0using Sqoop. Used Bash shell scripting to perform\u00a0Hadoop\u00a0operations. Designed the sequence diagrams to depict the data flow into Hadoop.  Involved in importing and exporting data between HDFS and Relational Systems like Oracle, Mysql, DB2 and Teradata using Sqoop. As a POC, extensively worked with Oozie workflow engine to run multiple Hive Jobs.\u00a0 Working on Hive to analyze the data and to extract report.  Involve in creating Hive tables, loading with data and writing Hive queries which will run internally in map reduce way. Developed Simple to complex MapReduce Jobs using Hive and Pig. Developed Shell and Python scripts to automate and provide Control flow to Pig scripts. Responsible for managing data from multiple sources.  Managing and scheduling Jobs on a\u00a0Hadoop\u00a0cluster using Oozie.\u00a0 \u00a0Developed Simple to complex Map/reduce Jobs using Java programming language that are implemented using Hive and Pig.\u00a0 Supported in setting up QA environment and updating configurations for implementing scripts with Pig, Hive and Sqoop\u00a0              Environment: Hadoop, Hive, Pig, Sqoop, Map Reduce, Linux, HDFS, Java. Java Developer       Client: iTwine Technologies, India                                                                                    Jan 2011 \u2013 Aug 2012                   Description: The main objective of the project is to develop a fully functional Ezee Mail System that enhances communications among the members of the organization in a reliable, cost-effective and secure way. Thus it helps the organization to improve the performance of its teams in the corporate sector.\u00a0 Responsibilities: Designed and developed the application using agile methodology. Implementation of new module development, new change requirement, fixes the code. Defect fixing for defects identified in pre-production environments and production environment. Wrote technical design document with class, sequence, and activity diagrams in each use case. Created Wiki pages using Confluence Documentation. Developed various reusable helper and utility classes which were used across all modules of application. Involved in developing XML compilers using XQuery. Use of MAVEN for dependency management and structure of the project Create the deployment document on various environments such as Test, QC, and UAT. Involved in system wide enhancements supporting the entire system and fixing reported bugs Explored Spring MVC, Spring IOC, Spring AOP, and Hibernate in creating the POC. Done data manipulation on front end using JavaScript and JSON.        Environment: Java, J2EE, JSP, Spring, Hibernate, CSS, JavaScript, Oracle, JBoss, Maven, Eclipse, JUnit, Log4J, AJAX, Web services, JNDI, JMS, HTML, XML, XSD, XML Schema, SVN, Git."}