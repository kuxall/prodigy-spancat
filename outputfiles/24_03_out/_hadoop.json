{"text": "Name: mary mary E-Mail: mary.mary@gmail.com Address: Kuala Lumpur, Malaysia Github: https://github.com/mary LinkedIn: https://linkedin.com/mary Phone No. 619839209666 ___________________________________________________________________________ Professional Summary: Over 7+years of IT experience in analysis, design and development using Hadoop, Java and J2EE. 2.5 years of experience with Hadoop, HDFS, Map Reduce and Hadoop Ecosystem (Pig & Hive). Have hands on experience in writing MapReducejobs using Java. Hands on experience in writing pig Latin scripts and pig commands.\u00a0 Hands on experience in installing, configuring and using ecosystem components like Hadoop MapReduce, HDFS, Sqoop, Pig & Hive Experience in database development using SQL and PL/SQL and experience working on databases like Oracle 9i/10g, Informix,  and SQL Server Experience working on NoSQL databases including Cassandra, MongoDB and Hbase.\u00a0 Experience using Sqoopto import data into HDFS from RDBMS and vice-versa. Extensive experience in Java and J2EE technologies like Servlets, JSP, JSF, JDBC, JavaScript, Ext JS,hibernate, and Junit testing. Experience in Restful web services using JAX-RS API Experience in implementing MVC, struts, and spring UNIX shell scripting, resource manager scheduler experience Expertise in developing web based GUIs using servlets, JSP, HTML, JavaScript, CSS Expertise in using J2EE application servers such as IBM Websphere 6.x/7.x, JBoss and web servers like apachetomcat. Experienced in java GUI/IDE Tools using Eclipse, RSA Experienced in database GUI/IDE Tools using TOAD, SQL Developer and Queryman Effective team player and excellent communication skills with insight to determine priorities, schedule work and meet critical deadlines. TECHNICAL SUMARY: Big Data  Ecosystem\t\t: Hadoop, Map Reduce, HDFS, HBase, Zookeeper, Hive, Pig, Sqoop,     \t\t\tOozie, Flume and Talend Java Technologies\t: Java 5,Java 6, JAXP, AJAX, I18N, JFC Swing, Log4j, Java Help API Methodologies\t\t: Agile, UML, Design Patterns  Database\t\t: Oracle 10g, DB2,MySQL, No sql (MongoDB) Application Server\t:Apache Tomcat 5.x 6.0, Jboss 4.0 Web Tools\t\t: HTML, Java Script, XML, DTD, Schemas, XSL, XSLT, XPath,  DOM, XQuery Tools\t\t\t: SQL developer, DB visualize IDE / Testing Tools\t:NetBeans, Eclipse, WSAD, RAD Operating System\t: Windows. Linux Scripts\t\t\t: Bash, Python, ANT Testing API\t\t: JUNIT PROFESSIONAL EXPERIENCE: \t BlueCross & BlueShield, Lansing, MI\t\t\t\t\t\t Apr\u20192013 \u2013 Till date Big Data / Hadoop Developer Project: Big Data as Service Description:Big Data as a service is a project particularly designed for serving the business partners which involves in collecting the data of all the historic claims and loads that into Big Data platform. It also involvesassigning a personal physician for each and everypatient respective to particular area.  Responsibilities: Have setup the 64 node cluster and configured the entire Hadoop platform. Migrating the needed data from MySQL& Mongo DB into HDFS using Sqoop and importing various formats of flat files into HDFS. Mainly worked on Hive queries to categorize data of different claims. Integrated the hive warehouse with HBase Written customized HiveUDFs in Java where the functionality is too complex. Designed and created Hive external tables using shared meta-store instead of derby with partitioning, dynamic partitioning and buckets. HiveQLscripts to create, load, and query tables in a\u00a0Hive.\u00a0 Generate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector. Supported Map Reduce Programs those are running on the cluster Maintain System integrity of all sub-components related to Hadoop. Maintained System integrity of all sub-components (primarily HDFS, MR, HBase, and Hive). Monitored System health and logs and respond accordingly to any warning or failure conditions. Presented data and dataflow using Talend for reusability. Environment:Apache Hadoop, HDFS, Hive, Map Reduce, Java, Pig, Sqoop, Cloudera CDH4, MySQL, Tableau, Talend, Elastic search, Kibana, SFTP John Hancock, Boston, MA\t\t\t\t\t\tJan 2012 \u2013 March 2013 Hadoop Developer Description: Existing Card management system is developed on data warehousing frame work, since data processing and data collection is time consuming and the enhancement is cumbersome. The New system has been built on Big data platform can process very large volume of data with low latency.Phase by phase the system designing is convert all the large volume of data process through the Hadoop frame work and the output/intermediate result will be transmitted to warehouse platform for reporting. The current phase involves fraudulent reports generation to meet customer demand. Data processing is done on Hadoop frame work to generate rule base various Fraud detection reports like Fraudulent customers, Fraudulent merchants etc\u2026 Responsibilities: Develop JAVA MapReduce Jobs for the aggregation and interest matrix calculation for users. Run clustering and user recommendation agents on the weblogs and profiles of the users to generate the interest matrix. Lead &Programmed the recommendation logic for various clustering and classification algorithms using JAVA. Experienced in managing and reviewing Hadoop log files Create and maintain Hive warehouse for Hive analysis. Run various Hive queries on the data dumps and generate aggregated datasets for downstream systems for further analysis. Use Apache Scoop to dump the data user data into the HDFS on a weekly basis. Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way Generate test cases for the new MR jobs. Prepare the data for consumption by formatting it for upload to the UDB system. Environment: HDFS, Apache Pig, MapReduce, Java, Hive,Sqoop, Text Analytics, Shell scripting Ally Financial (GMAC), MI                                                                      \t\tMay \u20192010 \u2013 Dec \u20182011 Sr.Java Developer Description: Ally Financial Inc., previously known as\u00a0GMAC Inc (General Motors Acceptance Corporation) is a\u00a0bank holding company\u00a0headquartered in\u00a0Detroit,\u00a0Michigan,\u00a0United States\u00a0at Tower 200 of the\u00a0Renaissance Center. The bank has more than 15 million customers worldwide and provides a range of financial services including auto financing, corporate financing, insurance, mortgage services, and online banking. The company's Global Automotive Services offer retail auto financing and leasing.  Responsibilities: Responsible for requirement gathering and analysis through interaction with end users. Lead the team in designing use-case diagrams, class diagram,interaction using UML model with Rational Rose. Designed and developed the application using various design patterns, such as session facade,business delegate and service locator. Worked on Maven build tool. Involved in developing JSP pages using Struts custom tags,JQuery and Tiles Framework. Used JavaScript to perform client side validations and Struts-Validator Framework for server-side validation. Good experience in Mule development. Developed Web applications with Rich Internet applications using Java applets,SilverLight,JavaFX. Involved in creating Database SQL and PL/SQL queries and stored Procedures. Implemented Singleton classes for property loading and static data from DB. Debugged and developed applications using Rational Application Developer (RAD). Developed a Web service to communicate with the database using SOAP. Developed DAO (data access objects) using Spring Framework 3. Deployed the components in to WebSphere Application server 7. Actively involved in backend tuning SQL queries/DB script. Worked in writing commands using UNIX,Shell scripting. Involved in developing other subsystems\u2019 server-side components. Production supporting using IBM clear quest for fixing bugs. Environment:Java EE 6, IBM WebSphere Application Server 7, Apache-Struts 2.0, EJB 3, Spring 3.2, JSP 2.0, WebServices, JQuery 1.7, Servlet 3.0, Struts-Validator, Struts-Tiles, Tag Libraries, ANT 1.5, JDBC, Oracle 11g/SQL, JUNIT 3.8, CVS 1.2, Rational clear case,Eclipse 4.2,JSTL,DHTML MetLife Insurance Somerset, NJ\t\t\t\t\t Jan\u20192009 - April\u20192010 Java J2EE Developer Project: Disability Income Maintenance and Enhancement Application Disability Income Application is used for the Underwriting and Administration of Disabilities products. This is a Maintenance and Enhancement Project for Individual and Institutional policies of MetLife. Responsibilities: Coded the business methods according to the IBM Rational Rose UML model. Extensively used Core Java, Servlets, JSP and XML. Used Struts 1.2 in presentation tier. Generated the Hibernate XML and Java Mappings for the schemas Used DB2 Database to store the system data Used Rational Application Developer (RAD) as Integrated Development Environment (IDE).  Used unit testing for all the components using JUnit. Used Apache log 4j Logging framework for logging of trace and Auditing. Used Asynchronous JavaScript and XML (AJAX) for better and faster interactive Front-End. Used IBM Web-Sphere as the Application Server. Used IBM Rational Clearcase as the version controller. Environments: Java 1.6, Servlets, JSP, Struts1.2, IBM Rational Application Developer (RAD) 6, Web sphere 6.0, iText, AJAX, Rational Clear case, Rational Rose, Oracle 9i, log4j. Client: Cirrus Global Solutions, Hyderabad, India\tJul\u20192007- Nov\u20192008 JAVA Developer Order Management Software Order Management Software manages the sales operation of organization. It manages the customers, orders, invoices and returns.  Responsibilities:  Involved in the complete SDLC software development life cycle of the application from requirement analysis to testing. Developed the modules based on struts MVC Architecture. Developed The UI using JavaScript, JSP, HTML, and CSS for interactive cross browser functionality and complex user interface. Created Business Logic using Servlets, Session beans and deployed them on Weblogic server. Used MVC struts framework for application design. Created complex SQL Queries, PL/SQL Stored procedures, Functions for back end.  Prepared the Functional, Design and Test case specifications. Involved in writing Stored Procedures in Oracle to do some database side validations.  Performed unit testing, system testing and integration testing Developed Unit Test Cases. Used JUnit for unit testing of the application. Provided Technical support for production environments resolving the issues, analyzing the defects, providing and implementing the solution defects. Resolved more priority defects as per the schedule. Environment: J2EE, JSP, PL/SQL, HTML, CSS, Struts, JUnit"}