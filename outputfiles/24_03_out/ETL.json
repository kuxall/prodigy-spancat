{"text": "Name: evelyn evelyn E-Mail: evelyn.evelyn@gmail.com Address: Abidjan, C\u00f4te d'Ivoire Github: https://github.com/evelyn LinkedIn: https://linkedin.com/evelyn Phone No. 979313694456 \t summary Over 5+ years of IT experience in Data Analyst/ETL Developer with solid knowledge of Data Architecture, Data Modeling, Data Extraction, Data Manipulation, Data Transformation, Data Mapping, Data Quality, Data Validation, Data Integration, Data Staging, Data Migration and Data Warehousing as an ETL Developer using Informatica. Extensive experience in Extraction, Transformation and Loading of data using Informatica from heterogeneous sources.  Extensive experience with Healthcare and Finance domains. Strong ability to analyze source systems and business requirements, identify and document business rules, design data architecture for cohesive decision support and prepare dimensional data models.  Extensive knowledge of Dimensional Data Modeling like Star and Snowflake schemas and knowledge in designing tools like Erwin and Power Designer.  Extensively worked in development of Informatica Mappings and Informatica Workflows.  Experience with Informatica Advanced Techniques \u2013 Dynamic Caching, Memory Management, Parallel Processing to increase Performance throughput. Extensive knowledge with Dimensional Data modeling, Star Schema/Snowflakes Schema, Fact and Dimension tables.  Extensively involved in Optimization and Tuning of mappings and sessions in Informatica by identifying and eliminating bottlenecks, memory management and parallel threading.  Experience in creating and using Stored Procedures, Functions, Triggers, Views, Synonyms, and Packages in SQL Server 2000/2005, Oracle 10g/9i/8i and DB2.  Involved in Performance/ Query tuning. Generation /interpretation of explain plans and tuning SQL to improve performance.  Skilled in Unix Shell Scripting and experience on different UNIX platforms.  Maintained outstanding relationship with Business Analysts and Business Users to identify information needs as per business requirements.  Experience in working in an onsite-offshore structure and effectively coordinated tasks between onsite and offshore teams.  Solid Modeling experience in Ralph Kimball Methodology, Bill Inmon Methodology, Star Schema, Snow Flake Schema, Fact Tables, Dimension Tables, Dimensional Data modeling, ERwin7x.  Experience in the full-cycle software development including requirements gathering, prototyping, and proof of concept, design, documentation, implementation, testing, maintenance and production support.  Expertise in DDL and DML, as well as using Aggregate Functions. Professional experience in data analysis and identifying issues, as well as report creation from several and complex data sources. Developed numerous SQL Queries for data Extraction. Proficient in Conceptual, Logical and Physical Modeling  In-depth knowledge and skills in designing and developing the complex mappings from various transformations like Source Qualifier, Joiner, Aggregator, Router, Filter, Expression, Lookup, Sequence Generator, Update Strategy, Sorter Transformation etc., Proficient in Agile Methodology, Logical Modeling, Physical Modeling, Dimensional Data Modeling, Star Schema, Snowflake Schema, FACT tables, Dimension tables.  Well experienced in Error Handling and Troubleshooting using various log files. Expertise in using UNIX and writing Perl and UNIX shell scripts. Superior communication, presentation, analytical and problem solving skill. Ability to work well with all levels of business. TECHNICAL SKILLS EDUCATION MSinManagementInformationSystems from University of South Florida (USF), Tampa, FL PROFESSIONAL EXPERIENCE American Family Insurance, Madison, WI\t\t\t\t\t\tMay 2013 \u2013 Present ETL Developer  The primary objective of the project was to construct an aggregated data warehouse (DWH) for analytics and reports used by business, risk management, asset management groups.During this project I have worked closely with the data warehouse development team, customers, business analysts to analyze operational data sources, determine data availability, define the data warehouse schema and develop ETL processes for the creation, maintenance, administration and overall support of the data warehouse. Responsibilities: Used transformations such as Source Qualifier, Aggregator, Expression, Lookup, Router, Filter, Update Strategy, Joiner, Transaction Control and Stored Procedure. Broadly used ETL to load data from multiple sources to Staging area (Oracle 9i) using Informatica PowerCenter 9.1. Worked with pre and post sessions, and extracted data from Transaction System into Staging Area.  Document data warehouse requirements such as data standards and quality rules. Source to target data mapping including capturing data acquisitions and transformation rules by analyzing different data tables using SQL queries and creating mapping requirements. Conduct workflow analysis, process diagram analysis and gap analysis to derive requirements for existing system term considerations, the long term considerations and its benefits. Conduct user interviews, gather requirements, analyze, and document business requirements based on the analysis of loan processing workflow model. Translate business requirements into data requirements. Design the business requirement for the project scope, more efficient ETL process and Software Development Life Cycle methodology. Utilized of Informatica IDQ 8.6.1 to complete initial data profiling and matching/removing duplicate data.  Tuning the Informatica Mappings for optimum performance. Replaced filters with Routers and SQL Overrides for heterogeneous sources.  Resolved quality issues using data profiling and data mapping functionalities using informatica data explorer.  Bench Mark testing of Informatica Power Center mappings to calculate the lead times for batch processing and tuning the mappings.  Participate in data analysis, and data dictionary and metadata management collaborating with business analysts, SMEs, ETL developers, data quality analysts and database administrators. Profile large datasets and data files using SQL as primary language to check the quality of the source data. Responsible for the Dimensional Data Modeling and populating the business rules using mappings into the Repository for Meta Data management.  Perform impact analysis on required changes to data and data structures and respond in an expeditious manner.  Source to target mapping from the ODS Legacy System to new EDW.  Capture different kinds of DQ validation rules to ensure that the EDW gets the cleansed data for report generation. Test the data using the logs generated after loading the data in to EDW. Conduct process improvements, status meetings, monitor and track progress of project following Agile methodology.  Tuned sources, targets, mappings and sessions to improve the performance of data load Designed Logical and Physical Modeling using Erwin Data Modeler tool. Did data migration of Informatica Mappings, Sessions, and Workflows to Data Integrator. Perform Data Archiving, conduct Data Analysis and receive Data Feed from updated Data Sources. Created data stores, project, jobs, and data flows using Data Integrator Involved in the development of Informatica mappings and mapplets and also tuned them for Optimum performance, Dependencies and Batch Design. Created Several Informatica Mappings to populate the data into dimensions and fact tables. In charge of converting data using SQL Server 2005 using SSIS. Worked cooperatively with team members to identify and resolve various issues relating Informatica and other database related issues. Designing mapping templates to specify high level approach. Environment: Informatica PowerCenter 8.1, Teradata, Oracle 10g/9i, PL/SQL, TOAD, SQL*Plus, Erwin, Windows, UNIX HP UX, Control M. ASG \u2013 DW (MUFG UNION BANK N.A.), Monterey Park, CA\t\t\tDec 2012 \u2013 Mar 2013 ETL Developer Administrative Services Group (ASG) is one of the several divisions under Union Bank that comprise of 4 different domain processes of Union Bank of Californias. It retains data to support the Business and Legal requirements as no other system in the bank retains data for more than couple of weeks. DW maintains the jobs and technology components to bring the data from Source systems. DW Downstream systems maintain the Jobs to pull the data. Data will not flow in Data warehouse if there are no consumers. DW does not manipulate the data into the data warehouse as data can be changed only in system of records. Responsibilities: Analyzed business requirements and worked closely with various application teams and business teams to develop ETL code. Designed and Developed ETL Mappings to extract data from SQL, Flat files, DB2 and Oracle to load the data into the target database. Developed new mappings with reusable transformations and mapplets based on business requirements. Extensively worked in the performance tuning of programs, ETL procedures and processes. Error checking and testing of the ETL procedures and programs using Informatica session log. Used SQL tools like TOAD to run SQL queries and validate the data. Worked extensively in Informatica Data Profiling which will help us  to understand the nature of the  source data.  Created mappings using transformations like Source Qualifier, Aggregator, Expression, Lookup, Router, Filter, Update Strategy and Joiner transformations.  Created reusable transformations and Mapplets and used them in mappings.  Used XML Parser transformation to parse the source xml message queue. Used the upsert operation to insert and update data in sales force target. Created and monitored workflows and task using Informatica PowerCenter Workflow Manager. Performed a gap analysis on the existing data model of the staging layer versus the requirement on Data Management Portal. Provided SQL queries to extract data from the Legacy System on alerted accounts and customers information to feed into the Portfolio Analyzer system. Analyzing feed metrics data as a result of the ETL process to come up with the efficient SLA time to provide to the Operation team. Creating a Monthly Release Notice to communicate and aware various business partners on upcoming releases and implementations. Created SSIS packages to extract data from OLTP to OLAP systems and Scheduled Jobs to call the packages and Stored Procedures. Used Informatica Analyst tool for data profiling to generate score cards. Performed Data Analysis and Data Validation by writing complex SQL queries. Worked on automation process to maintain Reference Data from various systems to refresh the data periodically. Provided pseudo code to ETL development team for capturing exceptions as a part of DQ process. Extensively used Erwin for data modeling and Dimensional Data Modeling.  Prepared S-T mapping document incorporating different business transforming logic for implementation of the ETL process. Created various documents such as Source-To-Target data mapping document and Unit Test Cases. Involved in Unit testing and Iterative testing to check whether data loads into sales force target are accurate, which was extracted from message queue. Created error processing to handle error records according to the business requirement.  Actively coordinated with testing team in the testing phase and helped the team to understand dependency chain of the whole project. Environment: Informatica Powercenter 9.5/9.1, UNIX, Oracle 11g/10g, SQL, MQ, Salesforce, DB2, SQL Developer, Toad and Windows 2000 / XP, AQT. Bank of America, Raleigh, NC\t\t\t\t\t\tJan 2011- Oct 2012\t ETL Developer Bank of America is all about providing people, companies and institutional investors the financial products and services they need to help achieve their goals at every stage of their financial lives. \u00a0It is the second largest\u00a0bank holding company\u00a0in the United States by assets.  Responsibilities: Actively involved in gathering requirements and acquiring application knowledge from Business Managers & Application Owners. Prepared the High-level Design document to provide an overview of the technical design specification system required for Application Enhancements. Involved in designing process flow for extracting the data across various systems interacting. Designed ETL Process using Informatica to load data from Flat Files, and Excel Files to target Oracle Data Warehouse database. Interacted with the business community and database administrators to identify the Business requirements and data realties. Created various transformations according to the business logic like Source Qualifier, Normalizer, Lookup, Stored Procedure, Sequence Generator, Router, Filter, Aggregator, Joiner, Expression and Update Strategy. Created design document Informatica mappings based on business requirement. Created Informatica mappings using various Transformations like Joiner, Aggregate, Expression, Filter and Update Strategy. Developed ETL routines using Informatica Power Center and created mappings involving transformations like Lookup, Aggregator, Ranking, Expressions, Mapplets, connected and unconnected stored procedures, SQL overrides usage in Lookups and source filter usage in Source qualifiers and data flow management into multiple targets using Routers. Used workflow manager for session management, database connection management and scheduled the jobs to run in the batch process. Written and used UNIX shell scripts extensively for scheduling and pre/post session management Prepared the Standard Operating Procedure (Knowledge Transfer) document, which provides necessary information, required for the Maintenance and Operation of the application. Provided data loading, monitoring, system support and general trouble shooting necessary for all the workflows involved in the application during its production support phase. Actively participated in data base testing like checking the constraints, correctness of the data, stored procedures, field size validation, etc. Environment: Informatica Power Center 8.6, Oracle 11g, PL/SQL, SQL Server 2005, T-SQL, Erwin 4.0, Win 2008, Perl scripts, , Flat files, Toad, ODBC. CitiGroup, Manhattan, NY\t\t\t\t\t\t\tAug 2009 - Dec 2010 Data Analyst CitiGroup is a global diversified financial services holding company whose business provides consumers, corporations, governments, and institutions with a board range of financial products and services. I was working in Data Analysts\u2019 team and worked closely with the development team and business analysts. Responsibilities: Designed and extracted data from different source systems, transforming the data, then mapping and loading the data into the corporate data warehouse. Ensured optimal performance tuning at various levels such as source, mappings, target and sessions. Analyzed and validated complex system requirements and existing business processes and information schemes. Designeddeveloped and implemented new programs and modified existing applications. Translated the business requirements into detailed technical design. Worked on Metadata Manager to generate the reports for comparing different source databases, to see the run time of long running sessions. Used advanced performance optimization techniques. Found the Informatica mappings bottlenecks and optimized mappings to get the best performance and tuned the SQL queries. Used Informatica debugging techniques to debug the mappings and used session log files and bad files to trace errors occurred while loading Described performance counters and used thread Statistics. Involved in performance tuning of mappings, transformations and (workflow) sessions to optimize session performance. Enabled the Integration Service to write to the targets, read source data and transform and move data efficiently. Developed several complex mappings in Informatica, a variety of PowerCenter transformations, Mapping Parameters, Mapping Variables, Mapplets & Parameter files in Mapping Designer using both the Informatica PowerCenter and IDQ. Developed workflow tasks like reusable Email, Event wait, Timer, Command, Decision. SQL Tuning and rewriting of SQL code to improve performance. Wrote PL/SQL stored Procedures and Functions for Stored Procedure Transformations.  Environment:Oracle 10g, SQL, TOAD, MS Access, UNIX, DB2, Windows, Erwin, Business Objects XI R3.1, Share Point, MS Excel, MS Visio, SQL Server, PL/SQL."}