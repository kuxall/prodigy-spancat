{"text": "Name: marie marie E-Mail: marie.marie@gmail.com Address: Bijie, China Github: https://github.com/marie LinkedIn: https://linkedin.com/marie Phone No. 188554617397 PROFESSIONAL SUMMARY:\t\t\t\t\t\t\t\t\t Professional qualified\u00a0Data Scientist/Data Analyst\u00a0with\u00a0over 8+ years\u00a0of experience in\u00a0Data Science\u00a0and\u00a0Analytics\u00a0including Artificial Intelligence/Deep Learning/Machine Learning, Data Mining and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including\u00a0data extraction, data cleaning, statistical modeling\u00a0and\u00a0data visualization\u00a0with large data sets of structured and unstructured data, created ER diagrams and schema. Experienced with machine learning algorithm such as logistic regression,\u00a0random forest, XGboost, KNN, SVM, neural network, linear regression, lasso regression\u00a0and k -\u00a0means Implemented\u00a0Bagging\u00a0and\u00a0Boosting\u00a0to enhance the model performance. Strong skills in statistical methodologies such\u00a0as A/B\u00a0test, experiment design,\u00a0hypothesis\u00a0test,\u00a0ANOVA Extensively worked on\u00a0Python 3.5/2.7\u00a0(Numpy, Pandas, Matplotlib, NLTK and Scikit-learn) Experience in implementing data analysis with various analytic tools, such as\u00a0Anaconda 4.0 Jupiter Notebook 4.X, R 3.0\u00a0(ggplot2, Caret, dplyr) and\u00a0Excel \u2026 Solid ability to write and optimize diverse SQL queries, working knowledge of\u00a0RDBMS\u00a0like\u00a0SQL Server 2008,\u00a0NoSql\u00a0databases like\u00a0MongoDB 3.2 Developed API libraries and coded business logic using C#, XML and designed web pages using .NET framework, C#, Python, Django, HTML, AJAX Strong experience in\u00a0Big Data\u00a0technologies like\u00a0Spark 1.6, Spark sql, pySpark, Hadoop 2.X, HDFS, Hive 1.X Experience in visualization tools like,\u00a0Tableau 9.X, 10.X\u00a0for creating dashboards Excellent understanding\u00a0Agile\u00a0and Scrum development methodology Used the version control tools like\u00a0Git 2.X\u00a0and build tools like\u00a0Apache Maven/Ant Ability to maintain a fun, casual, professional and productive team atmosphere Experienced the full software life cycle in\u00a0SDLC,\u00a0Agile, Devops\u00a0and Scrum methodologies including creating requirements, test plans. Skilled in Advanced Regression Modeling, Correlation, Multivariate Analysis, Model Building, Business Intelligence tools and application of Statistical Concepts. Proficient in Predictive Modeling, Data Mining Methods, Factor Analysis,\u00a0ANOVA,\u00a0Hypothetical testing, normal distribution and other advanced statistical and econometric techniques. Developed predictive models using Decision\u00a0Tree,\u00a0Random Forest,\u00a0Na\u00efve Bayes,\u00a0Logistic Regression,\u00a0Social Network Analysis,\u00a0Cluster Analysis, and Neural Networks. Experienced in Machine Learning and Statistical Analysis with\u00a0Python Scikit-Learn. Experienced in\u00a0Python\u00a0to manipulate data for data loading and extraction and worked with\u00a0python\u00a0libraries like\u00a0Matplotlib,\u00a0Numpy,\u00a0Scipy\u00a0and\u00a0Pandas\u00a0for\u00a0data analysis. Worked with complex applications such as\u00a0R,\u00a0SAS,\u00a0Matlab\u00a0and\u00a0SPSS\u00a0to develop neural network, cluster analysis. Strong\u00a0C#, SQL\u00a0programming skills, with experience in working with functions, packages and triggers. Skilled in performing\u00a0data parsing, data ingestion, data manipulation,\u00a0data architecture,\u00a0data modelling\u00a0and data preparation with methods including describe data contents, compute descriptive statistics of\u00a0data, regex, split and combine, Remap, merge, subset, reindex, melt and reshape. Experienced in Visual Basic for Applications and VB programming languages C#, .NET framework to work with developing applications. Worked with\u00a0NoSQL\u00a0Database including\u00a0Hbase,\u00a0Cassandra\u00a0and\u00a0MongoDB. Experienced in\u00a0Big Data with Hadoop, HDFS, MapReduce, and Spark. Experienced in Data Integration Validation and Data Quality controls for\u00a0ETL\u00a0process and\u00a0Data\u00a0Warehousing using\u00a0MS Visual Studio SSIS, SSAS, SSRS. Proficient in\u00a0Tableau\u00a0and\u00a0R-Shiny\u00a0data visualization tools to analyze and obtain insights into large datasets, create visually powerful and actionable interactive reports and dashboards. Automated recurring reports using\u00a0SQL\u00a0and Python and visualized them on\u00a0BI\u00a0platform like\u00a0Tableau. Worked in development environment like\u00a0Git\u00a0and\u00a0VM. Excellent communication skills. Successfully working in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner. EDUCATION: Bachelor of Computer Science. TOOLS AND TECHNOLOGIES: PROFESSIONAL EXPERIENCE:\t Description: J.B. Hunt Transport Services, Inc. is a trucking and transportation company that was founded by Johnnie Bryan Hunt, and based in the Northwest Arkansas city of Lowell.    Responsibilities: Performed Data Profiling to learn about behavior with various features of USMLE examinations of various student patterns. Evaluated models using Cross Validation, Log loss function,\u00a0ROC\u00a0curves and used\u00a0AUC\u00a0for feature selection and elastic technologies like Elastic Search, Kibana etc Addressed over fitting by implementing of the algorithm regularization methods like\u00a0L2\u00a0and\u00a0L1. Implemented statistical modeling with\u00a0XGBoost\u00a0machine learning software package using Python to determine the predicted probabilities of each model. Created master data for modeling by combining various tables and derived fields from client data and students LORs, essays and various performance metrics. Formulated a basis for variable selection and\u00a0Grid Search, KFold\u00a0for optimal\u00a0hyper parameters Utilized Boosting algorithms to build a model for predictive analysis of student\u2019s behaviour who took USMLE exam apply for residency. Used\u00a0numpy, scipy, pandas, nltk(Natural Language Processing Toolkit), matplotlib\u00a0to build the model. Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams. Application of various\u00a0Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics,\u00a0natural language processing(NLP), supervised and unsupervised,\u00a0regression models . Used\u00a0Principal Component Analysis\u00a0in feature engineering to analyze high dimensional data. Performed Data Cleaning, features scaling, features engineering using pandas and\u00a0numpy\u00a0packages in python and build models using deep learning frameworks. Created deep learning models using\u00a0Tensor flow\u00a0and\u00a0keras\u00a0by combining all tests as a single normalized score and predict residency attainment of students. Used XGB classifier if the feature is an categorical variable and XGB regressor for continuous variables and combined it using\u00a0Feature Union\u00a0and\u00a0Function Trans fomer methods of Natural Language Processing. Used\u00a0Onevs Rest\u00a0Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems. Implemented application of various machine learning algorithms and statistical modeling like\u00a0Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression\u00a0using Python to determine the accuracy rate of each model. Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior. Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub. Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework. Environment: Python 2.x,3.x, Hive, AWS, Linux, Tableau Desktop, Microsoft Excel, NLP, Deep learning frameworks such as TensorFLow, Keras, Boosting algorithms etc . Description: Common Securitization Solutions (CSS) is a joint venture owned by Fannie Mae and Freddie Mac and was established to build a universal platform \u2014 or Common Securitization Platform (CSP) \u2014 for the issuance and management of mortgage-backed securities. The CSP will help Fannie Mae and Freddie Mac provide even greater liquidity to the mortgage finance industry by increasing the efficiency of the securitization process. The entire housing chain, from borrowers to investors, will benefit from what we do for our customers.    Responsibilities: Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc. Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised,\u00a0regression models,\u00a0social network analysis, neural\u00a0networks, deep learning,\u00a0SVM, clustering to identify Volume using scikit-learn package in\u00a0python,\u00a0Matlab. Utilized\u00a0Spark, Snowflake, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, Tensor Flow, HBase, Cassandra, Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories. Created and connected SQL engine through C# to connect database, developed API libraries and business logic using C#, XML and Python Exploring DAG\u2019s, their dependencies and logs using\u00a0AirFlow\u00a0pipelines for automation Performed data cleaning and feature selection using MLlib package in\u00a0PySpark\u00a0and working with deep learning frameworks such as\u00a0Caffe,\u00a0Neon\u00a0etc Developed\u00a0Spark/Scala,\u00a0Python, R\u00a0for regular expression (regex) project in the\u00a0Hadoop/Hive\u00a0environment with\u00a0Linux/Windows\u00a0for big data resources. Used clustering technique\u00a0K-Means\u00a0to identify outliers and to classify unlabeled data. Created user friendly interface for quick view of reports by using C#, JSP, XML and developed expandable menu that show drilldown data on graph click Evaluated models using Cross Validation, Log loss function,\u00a0ROC\u00a0curves and used\u00a0AUC\u00a0for feature selection and elastic technologies like Elastic Search, Kibana etc Categorised comments into positive and negative clusters from different social networking sites using\u00a0Sentiment Analysis\u00a0and\u00a0Text Analytics Tracking operations using sensors until certain criteria is met using\u00a0AirFlow\u00a0technology. Responsible for different Data mapping activities from Source systems to\u00a0Teradata\u00a0using utilities like TPump, FEXP, MLOAD, BTEQ, FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags. Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semi-structured data. Addressed overfitting by implementing of the algorithm regularization methods like\u00a0L2\u00a0and\u00a0L1. Used Principal Component Analysis in feature engineering to analyze high dimensional data. Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior. Performed Multinomial Logistic\u00a0Regression,\u00a0Random forest,\u00a0Decision\u00a0Tree,\u00a0SVM\u00a0to classify package is going to deliver on time for the new route. Performed data analysis by using\u00a0Hive\u00a0to retrieve the data from\u00a0Hadoop cluster,\u00a0Sql\u00a0to retrieve data from\u00a0Oracle\u00a0database and used\u00a0ETL\u00a0for data transformation. Used\u00a0MLlib,\u00a0Spark's Machine learning library to build and evaluate different models. Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments. Performed Data Cleaning, features scaling, features engineering using pandas and\u00a0numpy\u00a0packages in python and build models using SAP Predictive Analytics. Developed\u00a0Map Reduce\u00a0pipeline for feature extraction using\u00a0Hive\u00a0and\u00a0Pig. Created Data\u00a0Quality Scripts\u00a0using\u00a0SQL\u00a0and\u00a0Hive\u00a0to validate successful data load and quality of the data. Created various types of data visualizations using Python and\u00a0Tableau /Spotfire. Communicated the results with operations team for taking best decisions. Collected data needs and requirements by Interacting with the other departments. Environment: Python 2.x, CDH5, HDFS, C#, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark. Description: WEX is a global leader in payments solutions. We simplify the complexities of payment systems across continents and industries\u2014including Fleet, Corporate Payments, and Health.    Responsibilities:  Involved in Design, Development and Support phases of Software Development Life Cycle (SDLC) Performed data ETL by collecting, exporting, merging and massaging data from multiple sources and platforms including SSRS/SSIS (SQL Server Integration Services) in SQL Server. Programming experience with .NET framework, C#, Visual Studio 2005/2008 to build web based, client/server architecture and to produce reports with C# and JSP. Worked with cross-functional teams (including data engineer team) to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop. Performed data cleaning and feature selection using MLlib package in PySpark. Performed partitional clustering into 100 by k-means clustering using Scikit-learn package in Python where similar hotels for a search are grouped together. Used Python to perform ANOVA test to analyze the differences among hotel clusters. Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model. Determined the most accurately prediction model based on the accuracy rate. Used text-mining process of reviews to determine customers' concentrations. Delivered analysis support to hotel recommendation and providing an online A/B test. Designed Tableau bar graphs, scattered plots, and geographical maps to create detailed level summary reports and dashboards. Developed hybrid model to improve the accuracy rate. Environment: Python, PySpark, C#, Tableau, MongoDB, Hadoop, SQL Server, SDLC, ETL, SSIS, recommendation systems, Machine Learning Algorithms, text-mining process, A/B test. Description: Rabobank is a Dutch multinational banking and financial services company headquartered in Utrecht, Netherlands. It is a global leader in food and agriculture financing and sustainability-oriented banking.    Responsibilities: Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications. Used Erwin and Visio to create 3NF and dimensional data models and published to the business users and ETL / BI teams. Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity. Managed full SDLC processes involving requirements management, workflow analysis, source data analysis, data mapping, metadata management, data quality, testing strategy and maintenance of the model. Created and maintained Logical and Physical models for the data mart. Created partitions and indexes for the tables in the data mart. Multiple phased projects to develop EDW (Enterprise Data Warehouse) and Data Marts to support Business Intelligence needs as per the requirements of the client. Performed data profiling and analysis applied various data cleansing rules designed data standards and architecture/ designed the relational models. Gathering, reviewing business requirements and Analyzing data sources from Excel/SQL for design. Development, testing, and production rollover of reporting and analysis projects within Tableau Desktop. Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects. Interacting with the Business Users for gathering design requirements and taking feedback on improvements. Developed SQL queries in SQL Server management studio, Toad and generated complex reports forth end users. Involved in extensive DATA validation by writing several complex SQL queries and Involved in back-end testing and worked with data quality issues. Environment: Erwin, Visio, 3NF, ETL, EDW, Data marts, SQL, SQL Server, Excel. Description: Atos is a global leader in digital transformation with 120,000 employees in 73 countries and annual revenue of \u20ac 13 billion. European number one in Cloud, Cybersecurity and High-Performance Computing, the Group provides end-to-end Orchestrated Hybrid Cloud, Big Data, Business Applications and Digital Workplace solutions through its Digital Transformation Factory, as well as transactional services through Worldline, the European leader in the payment industry.    Responsibilities: Queried data from SQL server, imported other formats of data and performed data checking, cleansing, manipulation and reporting using SAS (Base and Macro) and SQL. Built loss forecast models using multitude credit data, census data and insurance data.  Built claim duration model for worker's comp indemnity loss reserve using survival analysis.  Researched and developed fraud detection model strategy. Planned and documented strategy in white paper and presentation to management.  Used extreme value theory and generalized Pareto distribution to fit excess liability loss data.  Performed ad hoc data analysis and reporting using SAS and Excel.  Provide guidance, training and sharing SAS programming and predictive modeling methodologies to team members Description: People Tech is an emerging leader in the Enterprise Applications and IT Services marketplace. People Tech draws its expertise from strategic partnerships with technology leaders like Microsoft, Oracle, and SAP and combines that with the deep understanding of its employees.    Responsibilities: Analyze business information requirements and model class diagrams and/or conceptual domain models.  Gather & Review Customer Information Requirements for OLAP and building the data mart.  Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.  Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and OracleSQL.  Analyzed business process workflows and assisted in the development of ETL procedures for mapping data from source to target systems.  Worked with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. Responsible for defining the key identifiers for each mapping/interface  Responsible for defining the functional requirement documents for each source to target interface.  Document, clarify, and communicate requests for change requests with the requestor and coordinate with the development and testing team.  Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.  Enterprise Metadata Library with any changes or updates.  Document data quality and traceability documents for each source interface.  Establish standards of procedures. Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer"}