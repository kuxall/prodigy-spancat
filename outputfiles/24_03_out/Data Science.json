{"text": "Name: martha martha E-Mail: martha.martha@gmail.com Address: Handan, China Github: https://github.com/martha LinkedIn: https://linkedin.com/martha Phone No. 187609255994                                                                                                                                                        RESUME SUMMARY 5 years of experience in Artificial Intelligence, Machine Learning, Data Science, Data Mining, Algorithms and Data Structures  and Web Application  development..  Proficient in ETL tools and techniques,  Proficient in Quality Assurance Testing and Batch Processing using Python and Linux/Unix. Experienced in Python performance tuning techniques and tools. Experienced with both Python 2 and Python 3. Experience with mathematical and statistical Python Libraries such as pandas, skit-learn, NumPy, NLTK, Pytorch, SciPy, TensorFLow, Keras and software such as MATLAB and R.  Deployed, debugged and maintained complex, distributed software stacks, containing Apache Spark, Hadoop HDFS and IPython Notebook servers, on cloud-based AWS system by optimizing the stacks for best computational performance and stability. Extensive experience with advanced mathematics, statistics, applied machine learning and visualizing complex data and concepts to diverse audiences.  Expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data. Experienced in generator functions and generator expressions.  Hands on experience in using Tensorflow and Probabilistic Graphical methods(Bayesian and Gaussian networks) to create machine learning models.  Ability to document ML project requirements and assess deliverable timelines. Excellent working knowledge in UNIX and Linux shell environments using command line utilities.  Experienced with GPU computing and data mining. Experienced  using tools for MDM(Master Data Management) like Oracle MDM and Reltio Experienced in implementing data analytics solutions in AWS (EC2, EMR. S3), Google Cloud, Microsoft Azure. Used AWS Lambda Functions to trigger the code and managed custom performance and security along. Experienced in Agile, Scrum, Waterfall and Sprints methodologies.  Experienced with object-oriented programming (OOPS) concepts using Python, C++ and Java. Excellent skills in analyzing big and complex  data, data matching, identify anomalies , and providing usable insight to internal and external data activities and trend Proficiency in implementing Multi-threaded applications and using Super computer servers Web/Application Development with deep understanding of the technologies which involves analysis, testing, design, development, implementation and maintenance of various web based application using Python and Django.3+ Experienced in developing consumer-based custom features and application development using Python, Django, HTML5/CSS, XML/JSON,  JavaScript. Vigorous knowledge in progressing web applications and effectuating Model View Control architecture using Django web application framework. Good knowledge of web services with protocols  like SOAP, REST. Strong Communication and Presentation Skills substantiated in past assignments with developers, project managers, subject-matter experts, stakeholders, system implementers, and application end-users. Experienced with Unit Testing, System Integration Testing (SIT) and User Acceptance Testing (UAT). Good experience in error and exceptional handling. Good knowledge of NoSQL databases like MongoDB and Cassandra and as well as relational databases.  Experienced with version control systems like Git, SVN to keep the versions and configurations of the code organized. Hands-on experience in writing and reviewing requirements, architecture documents, test plans, design documents, quality analysis and audits. Good knowledge on Database Definition Language, Database Design, Data Warehouse Design. Experienced in designing the automation framework using Shell scripting. Proficient in research of current process and emerging technologies which need analytic models, data inputs and outputs, analytic metrics and user interface needs.  Quick learner and keen to adopt the cutting-edge technologies . SKILLS  Operating Systems: Linux, Windows, Mac Programming: Python 3.6/3.7, Java, MATLAB, C, C++,C#, R, Haskell, JavaScript, HTML, CSS, Latex Databases: SQL, PostgreSQL, MySQL, Oracle, NoSQL, MS Access, Hadoop(for big data) Version Control: Git, SVN Cloud Services: AWS, Azure, Google Cloud Data Analytics/visualization tool: R, Microsoft Power BI,Microstrategy, Tableau, d3.js Pentaho, STATA, NVivo, Informatica, Hadoop, Accumulo (Google big table concept), HBase, Spark, Qlick, Flume, MQ Services, Sqoop, Elastic Search, MapReduce, Amazon S3, Azure, Zeppelin, Yarn, Hive, Python IDE Machine Learning/AI: Artificial Neural Network, Convolution Neural Network Bayesian Network/BBN, Linear  Regression, Logistic Regression, Decisions Tree, Random forest, Pruning, k-NN, SVM, SVDK Clustering, Page Rank and PCA, MCA, MFC, Apriori and other data mining and Deep Learning Algorithms, Social Media Analytics, Sentimental analysis, Market Basket Analysis, Bagging, Boosting, Reinforcement Learning, Q Learning, Hidden Markov Models, Feature selection. Tools:  Visual Studio, IntelliJ, Pycharm, Android Studio, Putty, Filezilla, TFS, JIRA, Rally, Version1, HP ALM, Test Track Pro, Rational team Concert  Methodologies : Agile, Scrum, Waterfall WORK EXPERIENCE  Bank of America, Houston, NM                                                        April 2019 - Dec 2019 Python Developer The Bank of America Corporation is an American multinational investment bank and financial services company. Its primary financial services revolve around commercial banking, wealth management, and investment banking. Roles and Responsibilities  Followed of agile development (e.g. XP, SCRUM, Kanban,JIRA) and continues integration. Involved in decision making for using different design pattern and technology stack Worked on maintaining/updating real time trading tools using the concepts of real time table, websockets and  messaging middleware. Developed end to end approval system that are used by all the internal teams in bank for approving or rejecting permission for different application raised by user working on different teams. Created API for the existing system to connect different other applications and users Used Messaging Middleware concepts and tools to communicate between multiple applications and processes.  Performed unit testing, QA and worked with business partners to resolve any issues discovered during UAT. Worked on designing and building service layer for different applications and frontend tool by working with Flask, MySQL,Cassandra, Jinja2, AMPS (message queuing system) Followed the Test Driven Development paradigm to write bug free and scalable code  Increased the performance of the internal applications that uses websockets by analyzing and changing the way how data were sent and received.  Developed high availability real time messaging and notification system for existing ticking applications Writing automated scripts to pull the data from different systems and application to produce reports.  Monitored the different application by using Bank\u2019s internal monitoring tools. Implementing RESTFUL Web Services for data transportation between multiple systems.  Involved in building database Model, APIs and Views utilizing python (sqlalchemy) in order to build an interactive web based solution. Involved in reviewing the code with teams before shipping them to production  Participated in communication between the development team and the internal product stakeholders regarding the product\u2019s design goals and application\u2019s capabilities. Helped the team to migrate the code base from python 2 to 3. Developed and implemented standards for clean code that maintained modularity, clarity and portability. Learned and proficiently used internal bank tools and modules in short amount of time, Worked with global development teams on the same project. UNMBBER, Albuquerque, NM                                                 May  2018 - Dec 2018 Python Developer The UNM Bureau of Business and Economic Research (UNMBBER) is the recognized expert in providing socioeconomic data and forecasting in New Mexico. UNMBBER\u2019s research team provides economic forecasting as well as economic research services and data communication tools tailored to the needs of clients \u2013 public, private, nonprofit, and philanthropic \u2013 seeking to understand and shape public policy on the state, regional, and local levels. UNMBBER\u2019s services and research help leaders in New Mexico to understand, forecast and identify trends and changing economic markets across the state of New Mexico in order to inform decision making. Roles and Responsibilities  System administrator for servers on Amazon Web Services: Handling production and staging server instances, Server Database Management, Creating API node for new functionality Managed database of more than 10MM records of data by writing scripts and macros to automate data updating and cleaning process which saved monotonous working hours. Worked with tools like Beautiful Soup for web scraping data from different government, state, national parks and organization websites.  Responsible for architecting, designing, implementing and supporting of cloud based infrastructure and its solutions. Involved in writing Python API for Amazon Lambda to manage some of the AWS services. Performed data pre-processing to clean, eliminate outliers in data and conducted data exploration to detect correlation, trends and patterns in the data Imported the big socio-economic data files , created functions to read and join the files and generated data visualizations of state wise statistics of the data using Python libraries and d3.js for displaying in the web and connected all the visualization directly to the database Utilized PostgreSQL , data warehousing programs, Tableau, and other dashboard/visualization tool sets for data intelligence and analysis. Linked all the critical data to a common point of reference using Reltio(MDM) and sharing among different groups of researchers.  Designing, reviewing, implementing and optimizing data transformation processes in the Hadoop and Informatica ecosystems Led several big data machine learning initiatives involving the design, development and deployment of advanced machine learning algorithms that impacted more than 1000 local business. Predicted how sick leave would affect the local and small scale business in New Mexico in upcoming 10 years, by  leading the survey team to collect and clean the data and preparing the Machine learning model.  Worked with the research team to forecast the poverty rate of New Mexico in upcoming years by building the machine learning model for the large set of Census data of New Mexico.  Build the ML model to forecast how much a new company, that is being established, would increase the employment rate and GDP in the present economy.  Build sentiment analysis model using Natural Language Processing (using NLTK) for new startup in New Mexico are based on customer reviews.  Worked with several government client, like MRCOG  ( Mid-Region Council of Governments) by providing them with clean data set and visualization for regional development planning. Build the water level visualization for all the water resources in New Mexico by collecting the data from the on-site sensors that would update in every 5 minutes.  Worked parallely with the research team to forecast the impact of new start up in the economy of New Mexico.  DataRobot, New York, NY                                                           May  2016 - May 2018 Customer Facing Data Scientist DataRobot is an Automated Machine Learning product firm. Helped DataRobot acquire new clients working with decision makers, conducting Proof of Concepts, nurturing prospects into clients and developing collaborative culture Roles and Responsibilities Ensured alignment with key technology and business stakeholders across globally diverse, Agile teams. Helped Bank of America through building predictive models in FX, Fixed Income, Investment Banking, Research, Commercial Lending, Wholesale Credit, Client Relations, M&A. FX Volume Prediction: built time series models to predict daily FX volume for CLS at a minute level using data from primary exchanges - EBS, Reuters, BofA volume, bid and ask rates, spreads, VWAPs, simple and exponential moving averages, order book entries, etc. Used Bollinger Bands, MACD, market events, holidays for EUR/USD, USD/CAD, etc currency pairs. MASE values were impressive compared to a na\u00efve model. Customer Attrition Prediction: built a highly successful customer attrition predictive model with 80% accuracy on FICC electronic trading from Bloomberg terminals using time series, feature engineering with financial ratios, etc. Capital Review Committee Revenue Prediction: predicted yearly revenues for years 1 to 3 for the bank on 16 products ranging from Treasury, Advisory, Credit to FX, and had beat bankers estimates. Funded Loan Growth Prediction: developed predictive models for funded loan growth for the Corporate Banking group at Bank of America and improved prediction probability six times. Found key drivers and early indicators. Worked with Balyasny Asset Management (BAM) hedge fund, JPMC, TD Bank, GRA (Global Risk) at BAML Used Oracle MDM to share critical data among different departments and personal.  Built custom machine learning models on large datasets in use cases such as optimal capital allocation, commercial loan growth, customer attrition, market trend prediction for the bank. Built multi-class sentiment analysis models on bank\u2019s research reports using NLP and Spacy. Moved Machine Learning projects into production and created tangible value for the firms. Brought business insights showing feature interactions in ratings tables, prediction explanations. Built several workflows that combined data preprocessing steps with feature engineering, feature selection, model selection, hyper-parameter tuning, model stacking, blending, using cross validation to avoid overfitting, validating models with lift charts and ROC curves, explaining insights through feature importance analysis, partial dependency plots. Handled class imbalance and large datasets. Explored human \u2013 machine hybrid approaches. Captured trends, seasonality patterns through time series models such as ARIMA, used lag variables and sliding window techniques, feature engineered variables through iterations. Analyzed unstructured text in analyst reports, built sentiment analysis using TFIDF, NLP, Spacy. Balanced algorithm accuracy over speed in XGBoost, Random Forest, GLM, ENet Blender, Logistic. Worked with bank regulators on variable stress testing, parameter sensitivity analysis. Helped Humana insurance with Marketing mix optimization, Emergency Room attendance estimates. Developed Oil recovery models for Hunt Oil, and transport ETA predictions for Rail Inc. and BASF. Handled large scale transactional, trading, loan, hospital, transportation, oil production data. Evangelized Artificial Intelligence, Machine Learning through presentations, online webinars, blogs. Wrote popular blogs on Machine Learning and received company\u2019s special award on content creation.  Environment: Python, R, SKLearn, Time Series, ARIMA, Multiclass, Anomaly Detection, Feature Engineering, Imbalanced data, SQL, Hive, Hadoop, Tableau, Spacy, NLP, Spark, DataRobot, Eureqa, Nutonian Client: - FIVE9, CA                                                                    Feb 2015 to April 2016 ROLE:  PYTHON DEVELOPER    Five9 is the leading provider of cloud contact center software. Five9 software creates more successful customer interactions while increasing contact center productivity.  Roles and responsibilities: Extensively worked on developing UI components using Angular.js and JSON to interact Restful web services. Developed dynamic web pages using Python, Django Frameworks. Utilize SAS programming skills within protocol team and perform all programming required for clinical trial analysis and reporting. Worked on Django API for accessing the database Employed JDBC in persistent service to  connect to MySQL and perform database operations. Involved in redesigning the process, Analysis, coding, testing and implementing. Wrote scripts in Python for extracting data from HTML file using Python library Beautiful Soup. Experience on S3 bucket and object key, the deployment package, or key when creating a Lambda function. Using Kubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. Supported Apache Server on Linux Platform. Experience Kubernetes services, support, and tools are widely available. Created specific images using Python Imaging Library for the custom images used for each book. Model View Control architecture is implemented using Django Framework to develop web applications. Used IBM DB2 SQL stored procedures and UNIX Shell Scripts for importing/exporting Data and Conversions. Created Django dashboard with custom look and feel for end user after a careful study of Django admin site and dashboard. Worked on Jenkins continuous integration tool for deployment of project. Deployed the project into Jenkins using GIT version control system. Created Git repository and added to GitHub project. Developed Application to access JSON and XML from Restful, Web Services from consumer side using JavaScript and Ext.js. Implemented Business logic, worked on data exchange, processed XML and HTML using Python 2.7. Built the Silent Circle Management System (SCMC) in Django, Python, and AngularJS while integrating with infrastructure services. Managed, developed, and designed a dashboard control panel for customers and Administrators using Django, J, CSS, JavaScript, Bootstrap, jQuery and REST API calls. Knowledge on API give permission to LAMBDA to Details settings. Performed troubleshooting, fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team. Managing cookies SSL/HTTPS encryption How to store passwords Using secret questions Forgotten username/password functionality Use of nonce to prevent cross-site request  Data tables utilizing PyQt to display customer and policy information and add, delete, update customer records. Created Python scripts for data access and analysis (Scripts, Data Feeds, XLS, FIXML) to aid in process and system monitoring, and reporting. Performed QA testing whenever required with both Black Box and White Box testing. Worked as part of an Agile/Scrum based development team and exposed to TDD approach in developing applications. Used JIRA for Bug tracking and issue tracking."}