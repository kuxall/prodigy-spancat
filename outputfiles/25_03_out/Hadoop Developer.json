{"text": "Name: alice alice E-Mail: alice.alice@gmail.com Address: Nantong, China Github: https://github.com/alice LinkedIn: https://linkedin.com/alice Phone No. 62752416456 Professional Summary Over 7+ years of total IT experience including 2 years 8 months in Hadoop and BigData technologies.  Strong experience in using Hadoop eco-system components like HDFS, MapReduce, Oozie, Pig, Hive, Sqoop, Flume, Kafka, Impala, HBase, Zookeeper. Excellent understanding of both Classic MapReduce, YARN and their applications in BigData Analytics.  Experience in working with Spark and Storm. Experience in installing, configuring and maintaining the Hadoop Cluster including YARN configuration using Cloudera, Hortonworks.  Experience in benchmarking Hadoop Cluster to tune and obtain the best performance out of it. Familiar with all stages of Software Development Life Cycle, Issue Tracking, Version Control and Deployment.  Extensively worked in writing, tuning and profiling jobs in MapReduce, Advanced MapReduce using Java. Experience in writing MRUnit to test the correctness of MapReduce programs. Expertise in writing Shell-Scripts, Cron Automation and Regular Expressions.  Hands on experience in dealing with Compression Codecs like Snappy, BZIP2.  Implemented workflows in Oozie using Sqoop, MapReduce, Hive and other Java and Shell actions. In-depth knowledge of working with Avro and Parquet formats. Excellent knowledge of Data Flow Lifecycle and implementing transformations and analytic solutions. Extending Hive and Pig core functionality by writing Custom UDFs. Expertise in creating Custom Serdes in Hive.  Excellent knowledge in NoSQL databases like HBase, Cassandra and MongoDB. Expertise in implementing Data-Mining techniques like social network analysis and sentiment analysis.  Good Knowledge in importing/exporting data into R objects to other formats and Text-Mining with R. Working knowledge in Data Warehousing with ETL tools like IBM - DB2 Warehouse Edition.  Extensively worked on Database Applications using DB2, Oracle, MySQL, PL/SQL. Hands on experience in application development using Java, RDBMS. Strong experience as a senior Java Developer in Web/intranet, Client/Server technologies using Java, J2EE, Servlets, JSP, EJB, JDBC. Expertise in implementing Database projects which includes Analysis, Design, Development, Testing and Implementation of end-to-end IT solutions.  Worked on End-To-End implementation with Data warehousing team and Strong understanding of Data Warehousing concepts and exposure to Data Modeling, Normalization and Business Process Analysis. Experience in Object Oriented Analysis, Design (OOAD) and development of software using UML Methodology, good knowledge of J2EE design patterns and Core Java design patterns. Excellent working knowledge of popular frameworks like Struts, Hibernate, and Spring MVC. Experience in Agile Engineering practices. Excellent interpersonal and communication skills, creative, research-minded, technically competent and result-oriented with problem solving and leadership skills. Skills Experience Nike, Inc.\t \t\t\t\t\t\t\t\t         Beaverton, OR Hadoop Developer  Jan 2014-Present Involvement in design, development and testing phases of\u00a0Software Development Life Cycle. Involved in setting up the Hadoop cluster along with Hadoop Administrator. Installed and configured Hadoop Ecosystem components.  Responsible for requirements gathering and performing real time analytics on the incoming data.  Imported the data from Oracle source and populated it into HDFS using Sqoop. Developed a data pipeline using Kafka and Storm to store data into HDFS. Automated the process for extraction of data from warehouses and weblogs by developing work-flows and coordinator jobs in OOZIE. Performed transformations like event joins, filter bot traffic and some pre-aggregations using Pig. Developed MapReduce jobs to Convert data files into Parquet file format.  Included MRUnit to test the correctness of MapReduce programs. Executed Hive queries on Parquet tables to perform data analysis to meet the business requirements. Implemented a POC with Spark SQL to interpret complex Json records. Created table definition and made the contents available as a Schema-BackedRDD. Developed business specific Custom UDF's in Hive, Pig. Optimizing MapReduce code, pig scripts and performance tuning and analysis.  Exported the aggregated data onto Oracle using Sqoop for reporting on the Tableau dashboard. Environment: CDH, Eclipse,\u00a0Centos Linux, HDFS, MapReduce, Kafka, Storm, Parquet, Pig, Hive, Sqoop,  Spark, Oracle, Oozie, RedHat Linux, Tableau. BCBSM \t\t\t\t\t\t\t\t    Detroit, MI Hadoop Developer Jul 2012 - Dec 2013 Involved in installing cluster and Configuring Hadoop Ecosystem components.  Worked with Hadoop administrator in rebalancing blocks and decommissioning nodes in the cluster. Responsible to manage data coming from different sources. Extracted the data onto HDFS using Flume, Kafka. Imported  and exported data using Sqoop to load data from RDBMS to HDFS  and vice versa, on regular basis. Developed, Monitored and Optimized MapReduce jobs for data cleaning and preprocessing. Built data pipeline using Pig and MapReduce in Java. Implemented MapReduce jobs to write data into Avro format. Automated all the jobs for pulling the data and to load into Hive tables, using Oozie workflows. Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard. Developed custom Serde's specific to the requirement in Hive. Implemented Pattern matching algorithms with Regular Expressions, built profiles using Hive and stored the results in HBase. Used Maven to build the application. Implemented Unit Testing using MRUnit. Environment: HDP, HDFS, Flume, Kafka, Sqoop, Pig, Hive, MapReduce, HBase, Oozie, MRUnit, Maven, Avro, RedHat Linux, RDBMS. Century Links\t\t\t\t\t\t\t\t         Denver, CO Java/J2EE Developer Aug 2010 to Jun 2012 Responsible for Analysis, Design, Development and Integration of UI components with backend using J2EE\u00a0technologies such as Servlets,\u00a0Java\u00a0Beans, JSP, JDBC.\u00a0 Used Spring Framework 3.2.2 for transaction management and Hibernate3 to persist the data into the database.\u00a0 Developed JSP's for user interfaces, JSP's uses\u00a0Java\u00a0Beans objects to produce responses. Created controller Servlets for handling HTTP requests from JSP pages. Writing JavaScript functions for various validation purposes. Implemented the presentation layer using Struts2 MVC framework. Designed HTML Web pages utilizing JavaScript and CSS.\u00a0 Involved in developing distributed, transactional, secure and portable applications based on\u00a0Java using EJB technology.\u00a0 Deployed web applications in web-logic server by creating Data source and uploading jars.\u00a0 Created connection pool, Configured deployment descriptor specifying data environment.\u00a0 Implemented Multithread concepts in\u00a0Java\u00a0classes to avoid deadlocking.\u00a0 Involved in High Level Design and prepared Logical view of the application.\u00a0 Involved in designing and developing of Object Oriented methodologies using UML and created Use Case, Class, Sequence diagrams and also in complete development, testing and maintenance process of the application. Created Core\u00a0java\u00a0Interfaces and Abstract classes for different functionalities.\u00a0 Environment: Java /J2EE, CSS, AJAX, XML, JSP, JS, Struts2, Hibernate3, Spring Framework 3.2, Web Services,  EJB3, Oracle, J-Unit, Windows XP, Web-logic Application Server, Ant 1.8.2, Ecplise3.x, SOA tool. Virtusa Corporation\t\t\t\t\t\t\t\t    Hyderabad, AP, India Java Developer Feb 2008 - Jul 2010 Extensively involved in the design and development of JSP screens to suit specific modules. Converted the application\u2019s console printing of process information to proper logging technology using log4j. Developed the business components (in core Java) used in the JSP screens. Involved in the implementation of logical and physical database design by creating suitable tables, views and triggers. Developed related procedures and functions used by JDBC calls in the above components. Extensively involved in performance tuning of Oracle queries. Created components to extract application messages stored in xml files. Executed UNIX shell scripts for command line administrative access to oracle database and for scheduling backup jobs. Created war files and deployed in web server. Performed source and version control using VSS. Involved in maintenance support. Environment: JDK, HTML, JavaScript, XML, JSP, Servlets, JDBC, Oracle 9i, Eclipse, Toad, Unix Shell Scripting, MS Visual SourceSafe, Windows 2000. Education Bachelor of Technology, Major: Information Technology"}