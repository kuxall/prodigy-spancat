{"text": "Name: mary mary E-Mail: mary.mary@gmail.com Address: Hefei, China Github: https://github.com/mary LinkedIn: https://linkedin.com/mary Phone No. 507329143983 PROFESSIONAL SUMMARY Over 8 plus years of progressive hands-on experience in analysis, ETL processes, design and development of enterprise level data warehouse architectures, designing, coding, testing, and integrating ETL. Experience in Dimensional data modelling techniques, Slow Changing Dimensions (SCD), Software Development Life Cycle (SDLC) (Requirement Analysis, Design, Development & Testing), and Data warehouse concepts - Star Schema/Snowflake Modelling, FACT & Dimensions tables, Physical & Logical Data Modelling. Experienced in integration of various data sources like Oracle 11g,10g/9i/8i, MS SQL Server, XML files, Teradata, Netezza, Sybase, DB2, Flat files, into staging area and different target databases. Expertise in the ETL Tool Informatica and have extensive experience in Power Center Client tools including Designer, Repository Manager, Workflow Manager/ Workflow Monitor. Extensively worked with complex mappings using various transformations like Filter, Joiner, Router, Source Qualifier, Expression, Union, Unconnected / Connected Lookup, Aggregator, Stored Procedure, XML Parser, Normalize, Sequence Generator, Update Strategy, Reusable Transformations, User Defined Functions, etc. Experienced with Informatica Power Exchange for Loading/Retrieving data from mainframe systems. Extensively worked on Relational Databases Systems like Oracle11g/10g/9i/8i, MS SQL Server, Teradata and Source files like flat files, XML files. Worked with Informatica Data Quality (IDQ) toolkit, Analysis, data cleansing, data matching, data conversion, exception handling, and reporting and monitoring capabilities using IDQ. Experienced in mapping techniques for Type 1, Type 2, and Type 3 Slowly Changing Dimensions Strong experience in SQL, PL/SQL, Tables, Database Links, Materialized Views, Synonyms, Sequences, Stored Procedures, Functions, Packages, Triggers, Joins, Unions, Cursors, Collections, and Indexes in Oracle. Prepared BTEQ scripts to load data from Preserve area to Staging area. Worked on Teradata SQL Assistant querying the source/target tables to validate the BTEQ scripts Expertise in creating SQL Queries, PL/SQL packages, Functions, Stored Procedures, Views, Triggers and Cursors. Experience  in SAP Hyberis  Experience in programming using JAVA Sound knowledge of Linux/UNIX, Shell scripting, experience in command line utilities like pmcmd to execute workflows in non-windows environments. Designed complex Mappings and have expertise in performance tuning and slowly-changing Dimension Tables and Fact tables. Expertise in Informatica Data Quality (IDQ) tool installation and Standardization, Address Validation and Deduplication. Experience in importing/exporting data between different sources like Oracle/Access/Excel etc. using SSIS/DTS utility Design and Development of the ETL jobs for the application to load data within the organization to Salesforce.com which is the cloud based CRM system with various opportunities, events, forums, and activities. Have hands on experience in tools like Address Doctor which is used for Address validations Expertise in creating databases, users, tables, triggers, macros, views, stored procedures, functions, Packages, joins and hash indexes in Teradata database. Good Experience in working with Teradata utilities like BTEQ, Fast Export, Fast Load, Multi Load to export and load data to/from different source systems including flat files. Extensive experience using database tool such as SQL *Plus, SQL *Developer, Autosys and TOAD. Proficient in Oracle Tools and Utilities such as TOAD and SQL*Loader. Production oncall support for the complex High Available databases, checking logs, debugging, provided work around to sustain abends and supported the entire system for daily/Weekly/Monthly loads Identified and fixed bottlenecks and tuned the complex Informatica mappings for better Performance. Excellent analytical, problem solving, technical, project management, training, and presentation skills. TECHNICAL SKILLS PROFESSIONAL EXPERIENCE                                Client\t:  New day USA, Fulton,MD                                Role\t:    Sr. ETL Informatica/IDQDeveloper                               Duration\t:    March 2017 to Present Project Description: New Day USA is a leading U.S. Home loan franchise In Fultonregion. NewDay Financial, LLC operates as a mortgage lender that provides financial solutions to U.S. veteran homeowners and their families. It offers mortgage loans for refinancing, debt consolidating, and home improvement applications. It also provides VA home loans. The company was incorporated in 2005 and is based in Fulton, Maryland. New Day Financial, LLC operates as a subsidiary of Chrysalis Holdings, LLC. The purpose of the project is to maintain the historical data in Data Warehouse through Change Data Capture (CDC), Slowly Changing Dimensions. It also aimed at quick retrieval of data from Data Warehouse to meet customer needs in a dynamic way. Responsibilities: Good proficiency in Informatica Data Quality and Informatica power center. Extensively used ETL to load data using Power Center from source systems like Flat Files into staging tables   and load the data into the target database Oracle. Analyzed the existing systems and made a Feasibility Study. Involved in full software development lifecycle from the throughout the development and support finance industry using Informatica Power Center, Repository Manager, Designer, Workflow Manager, and Workflow Monitor. Experience in ETL processes utilizing Informatica Power center tools to design data conversions from a variety of relational, XML and flat file source systems to data warehouse/data marts.\u00a0\u00a0 Design reference data and data quality rules using IDQ and involved in cleaning the data using IDQ in Informatica Data Quality environment. Created mappings in Informatica Data Quality (IDQ) using Parser, Standardizer and Address Validator Transformations.  Extensively worked on Data migration, Data cleansing and Data Staging of operational sources using ETL processes and providing data mining features for data warehouses.  Experience in Data mining, finding correlations or patterns among fields in Data base. Used PL/SQL to create Packages, Functions, and Procedure. Used PL/SQL and SQL*Loader to create ETL packages for flat file loading and error capturing into log tables. Writing Complex SQL Queries and PL/SQL Procedures to Extract Data from various source Tables of Data Warehouse Responsible for troubleshooting and resolving issues related to system performance, Informatica applications, and data integrity. Developed several IDQ complex mappings in Informatica a variety of Power Center, transformations, Mapping Parameters, Mapping Variables, Mapp lets& Parameter files in Mapping Designer using Informatica Power Center. Used Informatica Power Exchange for loading/retrieving data from mainframe system Experienced in creating IDQ mappings using Labeler, Standardizer, Address Validator transformations with Informatica Developer and migrated to Informatica Power Center. Profiled the data using and performed Proof of Conceptfor\u00a0Informatica Data Quality\u00a0(IDQ). Design, document and configure the Informatica MDM Hub to support loading, cleansing of data. Worked on data cleansing and standardization using the cleanse functions in Informatica MDM& Worked on maintaining the master data using Informatica MDM Imported the mappings developed in data quality (IDQ) to Informatica designer.  Worked on the connection to SSIS to Autosys scheduling tool to schedule the packages and run the packages. Developed and modified jil files for Informatica workflows and SSIS packages and scheduled in Autosys scheduling tool. Worked on Informatica Analyst Tool IDQ, to get score cards report for data issues. Wrote UNIX Shell Scripts for Informatica Pre-Session, Post-Session and Autosys scripts for scheduling the jobs (work flows). Used Autosys for Scheduling the Informatica Workflows & have done testing using Autosys scheduling tool. Provided Knowledge Transfer to the end users and created extensive documentation on the design, development, implementation, daily loads, and process flow of the mappings. Environment: Informatica Power Center 10.1/9.6.1, Data Quality, UNIX, SQL, MDM, Autosys,Oracle 11g/10g,PL/SQL, SSIS,Informatica Power Exchange.  Client\t:    GE Healthcare   , Milwaukee,WI               Role\t:    Sr. InformaticaDeveloper               Duration\t:    March 2015 \u2013 Feb 2017 Project Description: GE HealthCare is in the business of better health and it touches the lives of patients in virtually every aspect of healthcare. This project was mainly focused on managing the database for medical services to understand the Claims, undertake trend analysis and provide better services to Existing customers &new customers.  Responsibilities: Developing the ETL components as well as Oracle procedures, functions & triggers. Defined Trust and validation rules for the base tables& created PL/SQL procedures to load data from Source Tables to Staging Tables  Created Oracle PL/SQL Cursors, Triggers, Functions and Packages Involved in implementing business logic with Oracle-stored procedures, functions and triggers using PL/SQL. Used IDQ transformation like labels, standardizing, proofing, parser, address doctor, Match, Exception transformations for standardizing, profiling and scoring the data. Worked on data cleansing and standardization using the cleanse functions in Informatica MDM.  Performed land process to load data into landing tables of MDM Hub using external batch processing for initial data load in hub store.  Scheduled the batch jobs in Autosys to automate the process. Designed and Developed SSIS Packages using various Control Flow and Data Flow items to Transform and load the Data from various Databases using SSIS. Worked on Teradata SQL Assistant querying the source/target tables to validate the BTEQ scripts Worked on Teradata Utilities like Fast-Load, Multi-Load & Fast-Export.\u00a0 Involved in writing Autosys jobs, JIL file for Box as well as Command jobs.  Created run books for job information which is scheduled on Autosys. Environment: Informatica Power Center,MDM, UNIX, Oracle, Shell,Hadoop, PL/SQL, Erwin Tidal,Autosys, Oracle 11g/10g, Java, ExtraData,Teradata.                      Client\t:  American Express, Plantation \u2013 FL                     Role\t:    ETL Developer                  Duration\t:    May 2013 \u2013 Feb 2015 Project Description This project is about reporting American express data based on multiple databases of the customer\u2019s credit performance globally to the bureaus and inquiries about new and existing customer\u2019s credit performance with other financial institutions from the bureaus in the respective markets. Responsibilities: Documented high and low-level design document specifications for source-target mapping, based on the transformation rules. Documented technical requirements for ETL process and Design documents for each source. Designed, Developed and Supported Extraction, Transformation and Load Process (ETL) for data migration. UsedIDQ's standardized plans for addresses and names clean ups.  Worked on IDQ file configuration at user's machines and resolved the issues & used IDQ to complete initial data profiling and removing duplicate data. Worked on Data mining to analyzing data from different perspective and summarizing into useful information. Implemented the data mining and data extraction techniques. Design reference data and data quality rules using IDQ and involved in cleaning the data using IDQ in Informatica Data Quality (IDQ) environment.   Creating JAVA User Exits to customize the Hub functionality. Created and designed an application using java and SQL to store, delete and retrieve data Monitored ETL process activity and utilization, with particular strengths in performance tuning highly transactional data integration package in both the development and production environments. Created and used the Normalizer Transformation to normalize the flat files in the source data.  Worked on Maestro job scheduling and Unix Scripting. Involved in finding production status by using Autosys commands. Developed UNIX shell scripts to run the pmcmd functionality to start and stop sessions, batches and scheduling workflows.  Experience in working with reporting team in building collection layer for reporting purpose. Environment: Informatica Power Center, IDQ, Teradata, Oracle11g/10g,Autosys,Java ,Flat File, Windows.                           Client\t:   NTT Data ,Hyderabad, INDIA                         Role\t:   ETL / InformaticaDeveloper                     Duration\t:   Mar 2011\u2013 April 13 Project Description This project deals with building a real-time view of enterprise level data. A decision support system is built to compare and analyze product prices, their quantities and Customers profiles. This Enterprise Data Warehouse (EDW) is used to deliver reports and information to sales and marketing management. Responsibilities: Designing the dimensional model and data load process using SCD Type 2 for the quarterly membership reporting purposes.  Derived the dimensions and facts for the given data and loaded them on a regular interval as per the business requirement.  Extracted data from multiple sources such as Oracle, XML, and Flat Files and loaded the transformed data into targets in Oracle, Flat Files.  Designing and coding the automated balancing process for the feeds that goes out from data warehouse.  Implement the automated balancing and control process which will enable the control on the audit and balance and control for the ETL code.  Improving the database access performance by tuning the DB access methods like creating partitions, using SQL hints, and using proper indexes.  All the jobs are integrated using complex Mappings including Mapplets and Workflows using Informatica power center designer and workflow manager. Use debugger in identifying bugs in existing mappings by analyzing data flow, evaluating transformations, and create mapplets that provides reusability in mappings. Analyzing the impact and required changes to incorporate the standards in the existing data warehousing design.  Following the PDLC process to move the code across the environments though proper approvals and source control environments&Source control using SCM. Environment:Informatica Power Center, Power Exchange, UNIX, Oracle 10g,\u00a0SQL Server 2008, SQL Assistant,  Tearadata,DB2.       Client \t: AspireInfo Labs, Hyderabad, INDIA                    Role:   \t :ETLDeveloper                     Duration\t:   Dec 2009 \u2013 Feb 2011 Responsibilities: Involved in analysis, design, development, test data preparation, unit and integration testing, Preparation of Test cases and Test Results Coordinating with client, Business and ETL team on development Developed Batch jobs using extraction programs using COBOL, JCL, VSAM, Datasets, FTP to Load Informatica tables  Involved in full project life cycle - from analysis to production implementation and support with emphasis on identifying the source and source data validation, developing logic, and transformation as per the requirement and creating mappings and loading the data into BI database. Based on the business requirements created Functional design documents and Technical design specification documents for ETL Process. Developing code to extract, transform, and load (ETL) data from inbound flat files and various databases into various outbound files using complex business logic. Most of the transformations were used like Source Qualifier, Aggregator, Filter, Expression, and Unconnected and connected Lookups, Update Strategy. Extensively worked with the Debugger for handling the data errors in the mapping designer. Created events and various tasks in the work flows using workflow manager. Responsible for tuning ETL procedures to optimize load and query Performance. Created automated shell scripts to transfer files among servers using FTP, SFTP protocols and download files. Expertise in creating control files to define job dependencies and for scheduling using Informatica. Environment: Informatica Powercenter , ETL, Business Objects, Oracle , PL/SQL. Education Details :"}